#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cluster Profiler Module (LLM-powered)
======================================
S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch v√† m√¥ t·∫£ ƒë·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng cluster
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Optional
import logging

logger = logging.getLogger(__name__)


class ClusterProfiler:
    """
    Ph√¢n t√≠ch v√† m√¥ t·∫£ ƒë·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng cluster s·ª≠ d·ª•ng LLM
    
    Features:
    - T√≠nh to√°n statistical profile cho m·ªói cluster
    - S·ª≠ d·ª•ng LLM (Gemini/OpenAI) ƒë·ªÉ generate m√¥ t·∫£ t·ª± nhi√™n
    - So s√°nh cluster v·ªõi overall population
    - T·∫°o actionable insights v√† recommendations
    """
    
    def __init__(self, llm_provider: str = 'gemini', api_key: Optional[str] = None):
        """
        Args:
            llm_provider: 'gemini' ho·∫∑c 'openai'
            api_key: API key (n·∫øu None, s·∫Ω ƒë·ªçc t·ª´ env variable)
        """
        self.llm_provider = llm_provider.lower()
        self.api_key = api_key
        self.llm_client = None
        self.cluster_profiles = {}
        
        self._initialize_llm()
    
    def _initialize_llm(self):
        """Initialize LLM client"""
        try:
            if self.llm_provider == 'gemini':
                import google.generativeai as genai
                import os
                
                api_key = self.api_key or os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')
                if not api_key:
                    raise ValueError("Gemini API key not found. Set GOOGLE_API_KEY or GEMINI_API_KEY environment variable.")
                
                genai.configure(api_key=api_key)
                # S·ª≠ d·ª•ng gemini-2.5-flash (model m·ªõi, thay th·∫ø gemini-1.5-flash)
                self.llm_client = genai.GenerativeModel('gemini-2.5-flash')
                logger.info("‚úì Initialized Gemini LLM (gemini-2.5-flash)")
                
            elif self.llm_provider == 'openai':
                from openai import OpenAI
                import os
                
                api_key = self.api_key or os.getenv('OPENAI_API_KEY')
                if not api_key:
                    raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable.")
                
                self.llm_client = OpenAI(api_key=api_key)
                logger.info("‚úì Initialized OpenAI LLM")
                
            else:
                raise ValueError(f"Unsupported LLM provider: {self.llm_provider}. Use 'gemini' or 'openai'")
                
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            logger.warning("‚ö† LLM not available. Will generate basic profiles without AI descriptions.")
            self.llm_client = None
    
    def calculate_cluster_statistics(self, df: pd.DataFrame, cluster_col: str = 'cluster') -> Dict:
        """
        T√≠nh to√°n statistics cho t·ª´ng cluster
        
        Args:
            df: DataFrame ch·ª©a data v·ªõi cluster labels
            cluster_col: T√™n column ch·ª©a cluster ID
            
        Returns:
            Dict v·ªõi cluster statistics
        """
        logger.info("Calculating cluster statistics...")
        
        # Get feature columns (exclude metadata)
        exclude_cols = ['userid', 'cluster', 'group']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        # Overall statistics
        overall_stats = {
            'mean': df[feature_cols].mean().to_dict(),
            'std': df[feature_cols].std().to_dict(),
            'median': df[feature_cols].median().to_dict()
        }
        
        # Per-cluster statistics
        cluster_stats = {}
        
        for cluster_id in sorted(df[cluster_col].unique()):
            cluster_df = df[df[cluster_col] == cluster_id]
            n_students = len(cluster_df)
            pct_students = (n_students / len(df)) * 100
            
            # Feature statistics
            cluster_mean = cluster_df[feature_cols].mean()
            cluster_std = cluster_df[feature_cols].std()
            cluster_median = cluster_df[feature_cols].median()
            
            # Compare to overall (z-score like)
            deviation_from_overall = {}
            for feat in feature_cols:
                if overall_stats['std'][feat] > 0:
                    z_score = (cluster_mean[feat] - overall_stats['mean'][feat]) / overall_stats['std'][feat]
                    deviation_from_overall[feat] = float(z_score)
                else:
                    deviation_from_overall[feat] = 0.0
            
            # Identify top distinguishing features (highest absolute deviation)
            top_features = sorted(deviation_from_overall.items(), 
                                 key=lambda x: abs(x[1]), 
                                 reverse=True)[:5]
            
            cluster_stats[int(cluster_id)] = {
                'cluster_id': int(cluster_id),
                'n_students': int(n_students),
                'percentage': float(pct_students),
                'feature_means': {k: float(v) for k, v in cluster_mean.items()},
                'feature_stds': {k: float(v) for k, v in cluster_std.items()},
                'feature_medians': {k: float(v) for k, v in cluster_median.items()},
                'deviation_from_overall': deviation_from_overall,
                'top_distinguishing_features': [
                    {
                        'feature': feat,
                        'z_score': float(z_score),
                        'interpretation': 'much higher' if z_score > 1.5 else 
                                        'higher' if z_score > 0.5 else
                                        'much lower' if z_score < -1.5 else
                                        'lower' if z_score < -0.5 else
                                        'similar'
                    }
                    for feat, z_score in top_features
                ]
            }
        
        self.cluster_profiles = {
            'overall_stats': overall_stats,
            'cluster_stats': cluster_stats,
            'n_clusters': len(cluster_stats),
            'total_students': len(df),
            'features_analyzed': feature_cols
        }
        
        logger.info(f"‚úì Calculated statistics for {len(cluster_stats)} clusters")
        return self.cluster_profiles
    
    def generate_llm_description(self, cluster_id: int) -> str:
        """
        S·ª≠ d·ª•ng LLM ƒë·ªÉ generate m√¥ t·∫£ t·ª± nhi√™n cho cluster
        
        Args:
            cluster_id: ID c·ªßa cluster c·∫ßn m√¥ t·∫£
            
        Returns:
            M√¥ t·∫£ b·∫±ng ti·∫øng Vi·ªát
        """
        if not self.llm_client:
            return "LLM not available. Using basic description."
        
        cluster_data = self.cluster_profiles['cluster_stats'][cluster_id]
        
        # Prepare prompt
        prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu gi√°o d·ª•c. H√£y ph√¢n t√≠ch v√† m√¥ t·∫£ ƒë·∫∑c ƒëi·ªÉm c·ªßa nh√≥m h·ªçc sinh sau:

**Th√¥ng tin nh√≥m:**
- S·ªë l∆∞·ª£ng: {cluster_data['n_students']} h·ªçc sinh ({cluster_data['percentage']:.1f}% t·ªïng s·ªë)
- Top 5 ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t:
{self._format_top_features(cluster_data['top_distinguishing_features'])}

**Y√™u c·∫ßu:**
1. ƒê·∫∑t t√™n cho nh√≥m n√†y (v√≠ d·ª•: "H·ªçc sinh xu·∫•t s·∫Øc", "H·ªçc sinh c·∫ßn h·ªó tr·ª£", v.v.)
2. M√¥ t·∫£ ƒë·∫∑c ƒëi·ªÉm h·ªçc t·∫≠p c·ªßa nh√≥m (2-3 c√¢u)
3. Ph√¢n t√≠ch ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu
4. ƒê·ªÅ xu·∫•t 2-3 h√†nh ƒë·ªông c·ª• th·ªÉ ƒë·ªÉ h·ªó tr·ª£/ph√°t tri·ªÉn nh√≥m n√†y

**ƒê·ªãnh d·∫°ng tr·∫£ v·ªÅ (JSON):**
{{
    "name": "T√™n nh√≥m",
    "description": "M√¥ t·∫£ ng·∫Øn g·ªçn",
    "strengths": ["ƒêi·ªÉm m·∫°nh 1", "ƒêi·ªÉm m·∫°nh 2"],
    "weaknesses": ["ƒêi·ªÉm y·∫øu 1", "ƒêi·ªÉm y·∫øu 2"],
    "recommendations": ["H√†nh ƒë·ªông 1", "H√†nh ƒë·ªông 2", "H√†nh ƒë·ªông 3"]
}}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m text kh√°c.
"""
        
        try:
            if self.llm_provider == 'gemini':
                response = self.llm_client.generate_content(prompt)
                result_text = response.text
            else:  # openai
                response = self.llm_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "You are an educational data analyst. Always respond in Vietnamese and return valid JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7
                )
                result_text = response.choices[0].message.content
            
            # Parse JSON from response
            result_text = result_text.strip()
            if result_text.startswith('```json'):
                result_text = result_text[7:]
            if result_text.startswith('```'):
                result_text = result_text[3:]
            if result_text.endswith('```'):
                result_text = result_text[:-3]
            result_text = result_text.strip()
            
            result = json.loads(result_text)
            return result
            
        except Exception as e:
            logger.error(f"LLM generation failed for cluster {cluster_id}: {e}")
            return self._generate_fallback_description(cluster_id)
    
    def _format_top_features(self, top_features: List[Dict]) -> str:
        """Format top features cho prompt"""
        lines = []
        for feat_data in top_features:
            feat = feat_data['feature']
            interp = feat_data['interpretation']
            z = feat_data['z_score']
            lines.append(f"  - {feat}: {interp} (z-score: {z:.2f})")
        return '\n'.join(lines)
    
    def _generate_fallback_description(self, cluster_id: int) -> Dict:
        """Generate basic description khi LLM kh√¥ng available"""
        cluster_data = self.cluster_profiles['cluster_stats'][cluster_id]
        
        # Simple rule-based naming
        pct = cluster_data['percentage']
        if pct > 40:
            name = "Nh√≥m ƒëa s·ªë"
        elif pct < 10:
            name = "Nh√≥m thi·ªÉu s·ªë"
        else:
            name = f"Nh√≥m {cluster_id + 1}"
        
        return {
            'name': name,
            'description': f"Nh√≥m g·ªìm {cluster_data['n_students']} h·ªçc sinh ({pct:.1f}%).",
            'strengths': ["C·∫ßn ph√¢n t√≠ch th√™m"],
            'weaknesses': ["C·∫ßn ph√¢n t√≠ch th√™m"],
            'recommendations': ["C·∫ßn ph√¢n t√≠ch chi ti·∫øt h∆°n ƒë·ªÉ ƒë∆∞a ra ƒë·ªÅ xu·∫•t ph√π h·ª£p"]
        }
    
    def profile_all_clusters(self, df: pd.DataFrame, cluster_col: str = 'cluster') -> Dict:
        """
        Ph√¢n t√≠ch v√† m√¥ t·∫£ t·∫•t c·∫£ c√°c cluster
        
        Args:
            df: DataFrame ch·ª©a data v·ªõi cluster labels
            cluster_col: T√™n column ch·ª©a cluster ID
            
        Returns:
            Dict v·ªõi cluster profiles
        """
        logger.info("="*70)
        logger.info("CLUSTER PROFILING WITH LLM")
        logger.info("="*70)
        
        # Calculate statistics
        self.calculate_cluster_statistics(df, cluster_col)
        
        # Generate LLM descriptions
        logger.info("\nGenerating AI-powered descriptions for each cluster...")
        
        for cluster_id in sorted(self.cluster_profiles['cluster_stats'].keys()):
            logger.info(f"\nüìä Analyzing Cluster {cluster_id}...")
            
            llm_description = self.generate_llm_description(cluster_id)
            self.cluster_profiles['cluster_stats'][cluster_id]['ai_profile'] = llm_description
            
            logger.info(f"  ‚úì Name: {llm_description.get('name', 'N/A')}")
            logger.info(f"  ‚úì Description: {llm_description.get('description', 'N/A')}")
        
        logger.info("\n" + "="*70)
        logger.info(f"‚úì Profiled {len(self.cluster_profiles['cluster_stats'])} clusters")
        logger.info("="*70)
        
        return self.cluster_profiles
    
    def save_profiles(self, output_dir: str):
        """
        L∆∞u cluster profiles
        
        Args:
            output_dir: Directory to save profiles
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Save JSON
        json_path = output_path / 'cluster_profiles.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.cluster_profiles, f, indent=2, ensure_ascii=False)
        logger.info(f"  ‚úì Saved: {json_path}")
        
        # Save human-readable report
        self._save_text_report(output_path)
    
    def _save_text_report(self, output_path: Path):
        """L∆∞u b√°o c√°o text d·ªÖ ƒë·ªçc"""
        text_lines = []
        text_lines.append("="*80)
        text_lines.append("CLUSTER PROFILING REPORT (AI-POWERED)")
        text_lines.append("="*80)
        text_lines.append(f"\nT·ªïng s·ªë h·ªçc sinh: {self.cluster_profiles['total_students']}")
        text_lines.append(f"S·ªë c·ª•m: {self.cluster_profiles['n_clusters']}")
        text_lines.append(f"S·ªë features ph√¢n t√≠ch: {len(self.cluster_profiles['features_analyzed'])}")
        
        for cluster_id in sorted(self.cluster_profiles['cluster_stats'].keys()):
            data = self.cluster_profiles['cluster_stats'][cluster_id]
            ai_profile = data.get('ai_profile', {})
            
            text_lines.append("\n" + "="*80)
            text_lines.append(f"CLUSTER {cluster_id}: {ai_profile.get('name', 'N/A')}")
            text_lines.append("="*80)
            text_lines.append(f"\nüìä Th·ªëng k√™:")
            text_lines.append(f"  ‚Ä¢ S·ªë l∆∞·ª£ng: {data['n_students']} h·ªçc sinh ({data['percentage']:.1f}%)")
            
            text_lines.append(f"\nüìù M√¥ t·∫£:")
            text_lines.append(f"  {ai_profile.get('description', 'N/A')}")
            
            text_lines.append(f"\nüí™ ƒêi·ªÉm m·∫°nh:")
            for strength in ai_profile.get('strengths', []):
                text_lines.append(f"  ‚Ä¢ {strength}")
            
            text_lines.append(f"\n‚ö†Ô∏è ƒêi·ªÉm y·∫øu:")
            for weakness in ai_profile.get('weaknesses', []):
                text_lines.append(f"  ‚Ä¢ {weakness}")
            
            text_lines.append(f"\nüí° ƒê·ªÅ xu·∫•t h√†nh ƒë·ªông:")
            for i, rec in enumerate(ai_profile.get('recommendations', []), 1):
                text_lines.append(f"  {i}. {rec}")
            
            text_lines.append(f"\nüîç Top 5 ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t:")
            for feat_data in data['top_distinguishing_features']:
                feat = feat_data['feature']
                interp = feat_data['interpretation']
                z = feat_data['z_score']
                text_lines.append(f"  ‚Ä¢ {feat}: {interp} (z-score: {z:.2f})")
        
        text_lines.append("\n" + "="*80)
        text_lines.append("END OF REPORT")
        text_lines.append("="*80)
        
        text_path = output_path / 'cluster_profiles_report.txt'
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(text_lines))
        logger.info(f"  ‚úì Saved: {text_path}")


if __name__ == '__main__':
    # Test
    logging.basicConfig(level=logging.INFO,
                       format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Create sample data
    np.random.seed(42)
    data = {
        'userid': range(1, 101),
        'feature1': np.random.rand(100),
        'feature2': np.random.rand(100),
        'feature3': np.random.rand(100),
        'cluster': np.random.choice([0, 1, 2], 100)
    }
    df = pd.DataFrame(data)
    
    # Profile clusters
    profiler = ClusterProfiler(llm_provider='gemini')
    profiles = profiler.profile_all_clusters(df)
    profiler.save_profiles('test_output')
