# üéì Q-LEARNING ADAPTIVE LEARNING SYSTEM - T·ªîNG H·ª¢P D·ª∞ √ÅN

## üìå T·ªîNG QUAN

H·ªá th·ªëng g·ª£i √Ω l·ªô tr√¨nh h·ªçc t·∫≠p th√≠ch ·ª©ng s·ª≠ d·ª•ng **Q-Learning**, hu·∫•n luy·ªán t·ª´ d·ªØ li·ªáu m√¥ ph·ªèng h√†nh vi h·ªçc sinh, ph√¢n c·ª•m theo nƒÉng l·ª±c h·ªçc t·∫≠p.

**M·ª•c ti√™u**: V·ªõi state hi·ªán t·∫°i c·ªßa h·ªçc sinh (c·ª•m, module, ti·∫øn ƒë·ªô, ƒëi·ªÉm s·ªë, h√†nh ƒë·ªông g·∫ßn nh·∫•t, stuck), h·ªá th·ªëng g·ª£i √Ω **top-K actions** t·ªëi ∆∞u ƒë·ªÉ h·ªçc ti·∫øp.

---

## üèóÔ∏è KI·∫æN TR√öC H·ªÜ TH·ªêNG

### **Pipeline T·ªïng Th·ªÉ**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. CLUSTER STUDENTS ‚îÇ ‚Üí Ph√¢n c·ª•m h·ªçc sinh
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. SIMULATE DATA    ‚îÇ ‚Üí M√¥ ph·ªèng trajectories
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. TRAIN Q-LEARNING ‚îÇ ‚Üí H·ªçc Q-table
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. API SERVICE      ‚îÇ ‚Üí G·ª£i √Ω qua REST API
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Components Chi Ti·∫øt**

```
core/
‚îú‚îÄ‚îÄ state_builder_v2.py      # X√¢y d·ª±ng state 6D
‚îú‚îÄ‚îÄ action_space.py           # Qu·∫£n l√Ω 37 actions
‚îú‚îÄ‚îÄ reward_calculator_v2.py   # T√≠nh reward theo cluster
‚îú‚îÄ‚îÄ qlearning_agent_v2.py     # Q-Learning agent
‚îú‚îÄ‚îÄ simulator_v2.py           # M√¥ ph·ªèng h√†nh vi h·ªçc sinh
‚îî‚îÄ‚îÄ student_context.py        # Track context h·ªçc sinh

services/
‚îú‚îÄ‚îÄ model_loader.py           # Load model & components
‚îú‚îÄ‚îÄ cluster_service.py        # D·ª± ƒëo√°n cluster
‚îî‚îÄ‚îÄ recommendation_service.py # G·ª£i √Ω actions

api_service.py                # FastAPI REST API
train_qlearning_redesigned.py # Training script
```

---

## üìä STATE REPRESENTATION (6 CHI·ªÄU)

### **ƒê·ªãnh Nghƒ©a State**

State vector **S = [cluster_id, module, progress, score, recent_action, stuck]**

| Dimension | T√™n | Gi√° tr·ªã | √ù nghƒ©a |
|-----------|-----|---------|---------|
| **0** | `cluster_id` | 0-4 | C·ª•m h·ªçc sinh (weak/medium/strong) |
| **1** | `current_module` | 0-36 | Module ƒëang h·ªçc (index) |
| **2** | `module_progress` | 0.25/0.5/0.75/1.0 | Ti·∫øn ƒë·ªô module (quartiles) |
| **3** | `avg_score` | 0.25/0.5/0.75/1.0 | ƒêi·ªÉm TB (quartiles) |
| **4** | `recent_action` | 0-5 | H√†nh ƒë·ªông g·∫ßn nh·∫•t |
| **5** | `is_stuck` | 0/1 | C√≥ b·ªã stuck kh√¥ng |

### **Action Type Mapping**

```python
ACTION_TYPES = {
    'watch_video': 0,      # Xem video b√†i gi·∫£ng
    'do_quiz': 1,          # L√†m b√†i quiz
    'mod_forum': 2,        # Tham gia forum th·∫£o lu·∫≠n
    'review_quiz': 3,      # Xem l·∫°i k·∫øt qu·∫£ quiz
    'read_resource': 4,    # ƒê·ªçc t√†i li·ªáu
    'do_assignment': 5     # L√†m b√†i t·∫≠p l·ªõn
}
```

### **Stuck Detection Rules**

H·ªçc sinh b·ªã coi l√† **stuck** (is_stuck = 1) khi:

1. **Qu√° nhi·ªÅu l·∫ßn th·ª≠**: `quiz_attempts > 3`
2. **M·∫•t qu√° nhi·ªÅu th·ªùi gian**: `time_on_module > 2 √ó median_time`
3. **ƒêi·ªÉm s·ªë li√™n t·ª•c th·∫•p**: `avg(recent_scores) < 0.5` (v·ªõi ‚â•2 ƒëi·ªÉm)

### **V√≠ D·ª• State**

```python
state = [4, 5, 1.0, 1.0, 3, 0]
# Gi·∫£i th√≠ch:
# - cluster_id=4 ‚Üí Strong cluster (h·ªçc sinh gi·ªèi)
# - module=5 ‚Üí ƒêang h·ªçc module index 5
# - progress=1.0 ‚Üí ƒê√£ ho√†n th√†nh 100% module
# - score=1.0 ‚Üí ƒêi·ªÉm TB 100%
# - recent_action=3 ‚Üí V·ª´a review_quiz
# - stuck=0 ‚Üí Kh√¥ng b·ªã stuck
```

### **State Space Size**

```python
Total states = 5 clusters √ó 37 modules √ó 4 progress √ó 4 scores √ó 6 actions √ó 2 stuck
             = 35,520 possible states
```

**Th·ª±c t·∫ø**: Ch·ªâ ~22% states xu·∫•t hi·ªán trong training (7,779/34,560)

---

## üéØ ACTION SPACE (37 ACTIONS)

### **Action Structure**

```python
{
    'id': 46,                    # Module ID t·ª´ Moodle
    'name': 'Video b√†i gi·∫£ng 1', # T√™n ho·∫°t ƒë·ªông
    'type': 'watch_video',       # Lo·∫°i action
    'section': 'Tu·∫ßn 1',         # Ph·∫ßn kh√≥a h·ªçc
    'purpose': 'content'         # M·ª•c ƒë√≠ch
}
```

### **Ph√¢n Lo·∫°i Actions**

| Lo·∫°i | S·ªë l∆∞·ª£ng | V√≠ d·ª• Module IDs |
|------|----------|------------------|
| **watch_video** | 8 | 46, 58, 66, 74... |
| **do_quiz** | 15 | 47, 59, 67, 75... |
| **read_resource** | 10 | 49, 61, 69, 77... |
| **do_assignment** | 4 | 55, 80, 81, 82 |

### **Dual ID System**

- **Action Index** (0-36): D√πng trong array/list
- **Module ID** (46-82): ID th·ª±c t·ª´ Moodle

**Mapping quan tr·ªçng**:
```python
# API ‚Üí Q-table: Convert index to module ID
module_ids = [action_space.get_action_by_index(i).id for i in indices]

# Q-table ‚Üí API: Convert module ID back to index
action_idx = action_space.get_action_by_id(module_id).index
```

---

## üí∞ REWARD CALCULATOR (10 COMPONENTS)

### **Cluster Classification**

```python
CLUSTER_THRESHOLDS = {
    'weak': (0.0, 0.6),      # avg_grade < 0.6
    'medium': (0.6, 0.8),    # 0.6 ‚â§ avg_grade < 0.8
    'strong': (0.8, 1.0)     # avg_grade ‚â• 0.8
}
```

**Cluster mapping**:
- Cluster 0 (0.411) ‚Üí weak
- Cluster 1 (0.812) ‚Üí medium
- Cluster 2 (0.854) ‚Üí strong
- Cluster 4 (0.875) ‚Üí strong
- Cluster 5 (0.658) ‚Üí medium

### **Reward Components**

#### **1. Base Score Reward** (√ó10-20)
```python
if outcome['score'] >= 0.7:
    reward += outcome['score'] * scale
    # scale: weak=10, medium=15, strong=20
```

#### **2. Progress Reward** (+5/+10)
```python
if outcome['completed']:
    reward += {weak: 5, medium: 7, strong: 10}
```

#### **3. Stuck State Penalty** (-5/-10/-15)
```python
if is_stuck == 1:
    reward -= {weak: 5, medium: 10, strong: 15}
```

#### **4. Challenge Bonus** (+3/+6/+10)
```python
if success and action_difficulty == 'hard':
    reward += {weak: 10, medium: 6, strong: 3}
```

#### **5. Time Efficiency** (+3, strong only)
```python
if cluster == 'strong' and time < expected_time * 0.8:
    reward += 3.0
```

#### **6. High Score Bonus** (+3/+5/+7)
```python
if score >= 0.9:
    reward += {weak: 7, medium: 5, strong: 3}
```

#### **7. Repetition Penalty** (-2/-3/-5)
```python
if same_action_twice:
    reward -= {weak: 2, medium: 3, strong: 5}
```

#### **8. Action Diversity Bonus** (+0.5/+1.0/+1.5)
```python
if recent_action != current_action_type:
    reward += {weak: 0.5, medium: 1.0, strong: 1.5}
```

#### **9. Beneficial Sequence Logic** (+0.7 to +2.6)
```python
BENEFICIAL_SEQUENCES = {
    (read_resource, quiz): 2.0,      # ƒê·ªçc ‚Üí l√†m quiz
    (watch_video, quiz): 1.5,        # Video ‚Üí quiz
    (quiz, review_quiz): 1.0,        # Quiz ‚Üí xem l·∫°i
    (read_resource, assignment): 1.5,
    (watch_video, assignment): 1.5,
    (forum, quiz): 1.0
}
# Scaled by cluster: weak√ó0.7, medium√ó1.0, strong√ó1.3
```

#### **10. Repetition Penalty (3x)** (-0.5 to -2.5)
```python
if same_action_3_times_in_row:
    reward -= {weak: 0.5, medium: 1.5, strong: 2.5}
```

### **Reward Range**

- **Minimum**: ~-20 (stuck + low score + repetition)
- **Maximum**: ~+40 (perfect score + sequence + diversity + challenge)
- **Typical**: +10 to +25 (normal learning progress)

---

## ü§ñ Q-LEARNING AGENT

### **Algorithm**

**Q-Learning Update Rule**:
```python
Q(s, a) ‚Üê Q(s, a) + Œ± √ó [r + Œ≥ √ó max Q(s', a') - Q(s, a)]
```

- **Œ± (learning_rate)**: 0.1 (cluster-adaptive)
- **Œ≥ (discount_factor)**: 0.95
- **Œµ (epsilon)**: 0.3 ‚Üí 0.01 (Œµ-greedy exploration)

### **Training Process**

```python
# 1. Load trajectories
trajectories = load_trajectories('data/simulated_trajectories_best.json')

# 2. Initialize agent
agent = QLearningAgentV2(
    n_actions=37,
    learning_rate=0.1,
    discount_factor=0.95,
    epsilon=0.3,
    cluster_adaptive=True
)

# 3. Train
agent.train(
    trajectories=trajectories,
    n_episodes=10000,
    verbose=True
)

# 4. Save Q-table
agent.save_qtable('models/qtable_best.pkl')
```

### **Training Results**

```
Episodes trained: 10,000
Total Q-updates: 1,415,430
Final epsilon: 0.01
Q-table size: 7,779 states
State coverage: 22.51% (7779/34560)
Training time: ~45 minutes
```

### **Q-Table Structure**

```python
q_table = {
    (4, 5, 1.0, 1.0, 3, 0): {  # State tuple
        46: 64.33,              # Module ID ‚Üí Q-value
        47: 48.94,
        49: 52.11,
        ...
    },
    ...
}
```

**Q-value Range**:
- Min: 0.0 (unexplored state-action)
- Max: 91.0 (highly rewarding path)
- Avg: ~30-50 (typical learned values)

---

## üîÑ SIMULATION (Data Generation)

### **Simulator Flow**

```
For each student (ID, cluster):
  1. Initialize state = [cluster, 0, 0.25, 0.5, 4, 0]
  2. Loop (max 100 steps):
     a. Agent picks action (Œµ-greedy)
     b. Simulate outcome (score, time, completed)
     c. Calculate reward
     d. Update state
     e. Record transition (s, a, r, s')
     f. If all modules done ‚Üí break
  3. Save trajectory
```

### **Student Behavior Model**

```python
# Success probability
success_prob = ability * (1 - difficulty) + current_score * 0.3

# Outcome simulation
if random() < success_prob:
    score = random(0.7, 1.0)
    time = expected_time * random(0.8, 1.2)
else:
    score = random(0.3, 0.6)
    time = expected_time * random(1.2, 2.0)
```

### **Cluster Parameters**

| Cluster | Level | Ability | Engagement | Avg Grade |
|---------|-------|---------|------------|-----------|
| 0 | Weak | 0.45 | 0.5 | 0.411 |
| 1 | Medium | 0.75 | 0.8 | 0.812 |
| 2 | Strong | 0.90 | 0.95 | 0.854 |
| 4 | Strong | 0.92 | 0.98 | 0.875 |
| 5 | Medium | 0.65 | 0.7 | 0.658 |

### **Generated Data Stats**

```
Total students: 400 (80 per cluster √ó 5 clusters)
Total transitions: 44,285
Avg trajectory length: 110.7 steps
Unique states: 7,779
Coverage: 22.51%
File: data/simulated_trajectories_best.json (9.8 MB)
```

---

## üåê API SERVICE

### **Endpoints**

#### **1. Health Check**
```http
GET /api/health

Response:
{
  "status": "healthy",
  "model_loaded": true,
  "n_states_in_qtable": 7779
}
```

#### **2. Model Info**
```http
GET /api/model-info

Response:
{
  "model_version": "V2",
  "episodes_trained": 10000,
  "total_updates": 1415430,
  "q_table_size": 7779,
  "n_actions": 37,
  "n_clusters": 5,
  "final_epsilon": 0.01
}
```

#### **3. Recommend** (Main API)
```http
POST /api/recommend

Request (Option 1 - G·ª≠i state tr·ª±c ti·∫øp):
{
  "student_id": "SV001",
  "state": [4, 5, 1.0, 1.0, 3, 0],
  "top_k": 3
}

Request (Option 2 - G·ª≠i features, API t·ª± build state):
{
  "student_id": "SV001",
  "features": {
    "avg_grade": 0.875,
    "completion_rate": 0.95,
    "quiz_scores": [0.8, 0.9, 0.85]
  },
  "top_k": 3
}

Response:
{
  "success": true,
  "student_id": "SV001",
  "cluster_id": 4,
  "cluster_name": "Strong learner",
  "state_vector": [4, 5, 1.0, 1.0, 3, 0],
  "state_description": {
    "cluster": "Strong",
    "module": "Module 5",
    "progress": "100%",
    "score": "100%",
    "recent_action": "review_quiz",
    "stuck": false
  },
  "recommendations": [
    {
      "rank": 1,
      "action_id": 46,
      "action_index": 0,
      "name": "Video b√†i gi·∫£ng ti·∫øp theo",
      "type": "watch_video",
      "q_value": 64.33,
      "section": "Tu·∫ßn 2"
    },
    {
      "rank": 2,
      "action_id": 49,
      "action_index": 3,
      "name": "ƒê·ªçc t√†i li·ªáu",
      "type": "read_resource",
      "q_value": 52.11,
      "section": "Tu·∫ßn 2"
    },
    {
      "rank": 3,
      "action_id": 47,
      "action_index": 1,
      "name": "Quiz ki·ªÉm tra",
      "type": "do_quiz",
      "q_value": 48.94,
      "section": "Tu·∫ßn 2"
    }
  ],
  "model_info": {
    "model_version": "V2",
    "n_states_in_qtable": 7779
  }
}
```

---

## üìÇ INPUT/OUTPUT FILES

### **Input Data**

#### **1. Course Structure** (`data/course_structure.json`)
```json
{
  "id": 2,
  "fullname": "L·∫≠p tr√¨nh Web",
  "contents": [
    {
      "id": 7,
      "name": "Tu·∫ßn 1",
      "modules": [
        {
          "id": 46,
          "name": "Video b√†i gi·∫£ng 1",
          "modname": "hvp",
          "visible": 1,
          "uservisible": true
        }
      ]
    }
  ]
}
```

#### **2. Cluster Profiles** (`data/cluster_profiles.json`)
```json
{
  "n_clusters": 6,
  "cluster_stats": {
    "0": {
      "avg_grade": 0.411,
      "student_count": 145
    },
    "1": {
      "avg_grade": 0.812,
      "student_count": 203
    }
  }
}
```

#### **3. Simulated Trajectories** (`data/simulated_trajectories_best.json`)
```json
{
  "1000": [
    {
      "state": [0, 0, 0.25, 0.5, 4, 0],
      "action": 46,
      "reward": 12.5,
      "next_state": [0, 0, 0.5, 0.6, 0, 0],
      "is_terminal": false,
      "timestamp": "2024-01-01T10:00:00",
      "module_id": 46
    }
  ]
}
```

### **Output Models**

#### **1. Q-Table Model** (`models/qtable_best.pkl`)
```python
{
    'q_table': {
        (4, 5, 1.0, 1.0, 3, 0): {46: 64.33, 47: 48.94, ...},
        ...
    },
    'config': {
        'n_actions': 37,
        'learning_rate': 0.1,
        'discount_factor': 0.95
    },
    'stats': {
        'episodes_trained': 10000,
        'total_updates': 1415430,
        'final_epsilon': 0.01,
        'q_table_size': 7779
    }
}
```

---

## üöÄ C√ÅCH S·ª¨ D·ª§NG

### **1. Setup Environment**

```bash
# Clone repo
cd step7_qlearning

# Install dependencies
pip install -r requirements.txt
```

### **2. Generate Training Data**

```bash
# M√¥ ph·ªèng 400 h·ªçc sinh (80/cluster √ó 5 clusters)
python3 generate_large_simulation_data.py --preset production

# Output: data/simulated_trajectories_best.json
```

### **3. Train Q-Learning Model**

```bash
# Train v·ªõi 10,000 episodes
python3 train_qlearning_redesigned.py

# Output: models/qtable_best.pkl
# Training time: ~45 minutes
```

### **4. Start API Server**

```bash
# Start FastAPI server
uvicorn api_service:app --reload --port 8080

# Server running at: http://localhost:8080
# API docs: http://localhost:8080/docs
```

### **5. Test API**

```bash
# Test v·ªõi state vector
curl -X POST http://localhost:8080/api/recommend \
  -H "Content-Type: application/json" \
  -d '{
    "student_id": "SV001",
    "state": [4, 5, 1.0, 1.0, 3, 0],
    "top_k": 3
  }'

# Ho·∫∑c test v·ªõi Python
python3 test_qtable_api.py
```

---

## üìä K·∫æT QU·∫¢ & ƒê√ÅNH GI√Å

### **Model Performance**

| Metric | Value |
|--------|-------|
| **Q-table Coverage** | 22.51% (7,779/34,560 states) |
| **Avg Actions/State** | 1.95 actions |
| **States with Q>0** | 2,847 states (36.6%) |
| **Max Q-value** | 91.0 |
| **Training Episodes** | 10,000 |
| **Convergence** | Œµ: 0.3 ‚Üí 0.01 |

### **Top 5 Most Valuable States**

```python
State (4, 22, 1.0, 1.0, 3, 0):
  Max Q-value: 91.0
  Actions learned: 37
  Interpretation: Strong student, module 22, 100% progress/score

State (2, 34, 0.75, 1.0, 0, 0):
  Max Q-value: 88.5
  Actions learned: 35
```

### **Recommendation Quality**

**Test Case**: Strong student (cluster 4), 100% progress
```
Input state: [4, 5, 1.0, 1.0, 3, 0]

Top-3 Recommendations:
1. watch_video (Q=64.33) - H·ªçc n·ªôi dung m·ªõi
2. read_resource (Q=52.11) - ƒê·ªçc t√†i li·ªáu
3. do_quiz (Q=48.94) - Ki·ªÉm tra hi·ªÉu bi·∫øt

‚úì Logic: Video ‚Üí Reading ‚Üí Quiz (beneficial sequence)
‚úì Diversity: 3 lo·∫°i action kh√°c nhau
‚úì No repetition: Kh√¥ng tr√πng recent_action
```

### **API Performance**

- **Latency**: ~50-100ms/request
- **Throughput**: ~100 requests/second
- **Model size**: 2.8 MB (qtable_best.pkl)
- **Memory usage**: ~150 MB (loaded in RAM)

---

## üêõ CRITICAL BUGS FIXED

### **Bug 1: Q-value Always 0.0**

**V·∫•n ƒë·ªÅ**: Recommendations tr·∫£ v·ªÅ q_value=0.0 d√π state c√≥ max_q=91.0

**Nguy√™n nh√¢n**: 
- Q-table d√πng **module IDs** (46, 47...) l√†m keys
- Code g·ª≠i **action indices** (0, 1, 2...) ‚Üí `q_table.get(0)` = None

**Gi·∫£i ph√°p**:
```python
# OLD (broken)
q_values = {idx: agent.q_table[state].get(idx, 0) for idx in [0,1,2...]}

# NEW (fixed)
module_ids = [action_space.get_action_by_index(i).id for i in [0,1,2...]]
q_values = {mid: agent.q_table[state].get(mid, 0) for mid in module_ids}
```

### **Bug 2: max(q_values) Wrong**

**V·∫•n ƒë·ªÅ**: `max(q_values)` tr·∫£ v·ªÅ max KEY thay v√¨ max VALUE

**Gi·∫£i ph√°p**:
```python
# OLD: max(q_values) ‚Üí max KEY (e.g., 91 if module_id=91)
# NEW: max(q_values.values()) ‚Üí max Q-VALUE
```

---

## üìö T√ÄI LI·ªÜU THAM KH·∫¢O

### **File Documents Quan Tr·ªçng**

1. **API_INPUT_CURRENT.md** - Format API input hi·ªán t·∫°i
2. **QTABLE_API_DOCS.md** - T√†i li·ªáu API chi ti·∫øt
3. **TRAINING_SUCCESS_REPORT.md** - B√°o c√°o k·∫øt qu·∫£ training
4. **ENHANCED_SIMULATOR_DOCS.md** - H∆∞·ªõng d·∫´n simulator

### **Core Modules**

- `core/qlearning_agent_v2.py` (539 lines) - Q-Learning algorithm
- `core/state_builder_v2.py` (404 lines) - State representation
- `core/reward_calculator_v2.py` (440 lines) - Reward function
- `core/simulator_v2.py` (1000+ lines) - Student simulator

### **Training & Deployment**

- `train_qlearning_redesigned.py` - Training script
- `api_service.py` - FastAPI server
- `generate_large_simulation_data.py` - Data generation

---

## üîÆ FUTURE IMPROVEMENTS

1. **Multi-objective Rewards**: C√¢n b·∫±ng completion rate vs retention
2. **Transfer Learning**: H·ªçc t·ª´ cluster n√†y sang cluster kh√°c
3. **Online Learning**: C·∫≠p nh·∫≠t Q-table real-time t·ª´ user feedback
4. **Deep Q-Network**: Thay Q-table b·∫±ng neural network
5. **Personalization**: H·ªçc ri√™ng cho t·ª´ng h·ªçc sinh (kh√¥ng ch·ªâ cluster)

---

## üìû SUPPORT

**Author**: Q-Learning V2 Team  
**Version**: 2.0  
**Last Updated**: November 2025

**Quick Commands**:
```bash
# Generate data
python3 generate_large_simulation_data.py --preset production

# Train model
python3 train_qlearning_redesigned.py

# Start server
uvicorn api_service:app --reload --port 8080

# Test API
python3 test_qtable_api.py
```

---

**üéØ T√ìM T·∫ÆT**: H·ªá th·ªëng Q-Learning ho√†n ch·ªânh v·ªõi 7,779 states ƒë√£ h·ªçc, 10 reward components th√¥ng minh, API REST nhanh (~100ms), v√† kh·∫£ nƒÉng g·ª£i √Ω actions t·ªëi ∆∞u d·ª±a tr√™n cluster + state hi·ªán t·∫°i c·ªßa h·ªçc sinh! üöÄ
