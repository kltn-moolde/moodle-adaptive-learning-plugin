# ðŸ“Š HÆ°á»›ng Dáº«n Äá»c Biá»ƒu Äá»“ Training Q-Learning

## ðŸŽ¯ Má»¥c ÄÃ­ch
CÃ¡c biá»ƒu Ä‘á»“ giÃºp báº¡n theo dÃµi quÃ¡ trÃ¬nh training vÃ  Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng model.

---

## 1. ðŸ“ˆ Training Metrics (training_metrics.png)

### ðŸ”¹ Subplot 1: Average Reward per Epoch
**Ã nghÄ©a**: Äiá»ƒm sá»‘ trung bÃ¬nh mÃ  agent nháº­n Ä‘Æ°á»£c

```
Tá»‘t: TÄƒng dáº§n vÃ  á»•n Ä‘á»‹nh
Xáº¥u: Giáº£m dáº§n hoáº·c dao Ä‘á»™ng máº¡nh
```

**VÃ­ dá»¥ cá»§a báº¡n**: 
- Reward = 69.267 (constant)
- âœ… Tá»‘t: Model Ä‘Ã£ converge ngay, khÃ´ng cáº§n train thÃªm
- âš ï¸ LÆ°u Ã½: Reward khÃ´ng Ä‘á»•i cÃ³ thá»ƒ do:
  - Data Ä‘Ã£ Ä‘Æ°á»£c train trÆ°á»›c Ä‘Ã³
  - Model Ä‘Ã£ há»c háº¿t patterns trong data
  - Exploration rate (epsilon) quÃ¡ tháº¥p

### ðŸ”¹ Subplot 2: Q-Table Size Growth
**Ã nghÄ©a**: Sá»‘ lÆ°á»£ng states mÃ  model Ä‘Ã£ gáº·p

```
Tá»‘t: TÄƒng dáº§n rá»“i flatten
Xáº¥u: TÄƒng khÃ´ng ngá»«ng (overfitting)
```

**VÃ­ dá»¥ cá»§a báº¡n**:
- Size = 35,366 states (constant)
- âœ… Ráº¥t tá»‘t: Q-table Ä‘Ã£ Ä‘á»§ lá»›n Ä‘á»ƒ cover nhiá»u tÃ¬nh huá»‘ng
- ðŸ“Š So sÃ¡nh: TÄƒng 1200% so vá»›i training trÆ°á»›c (2,717 â†’ 35,366)

### ðŸ”¹ Subplot 3: Average Q-Value per Epoch
**Ã nghÄ©a**: GiÃ¡ trá»‹ trung bÃ¬nh cá»§a táº¥t cáº£ Q-values

```
Tá»‘t: TÄƒng dáº§n vÃ  stable
Xáº¥u: Q-values = 0 hoáº·c quÃ¡ lá»›n (>1000)
```

**VÃ­ dá»¥ cá»§a báº¡n**:
- Start: 0.413 â†’ End: 5.315
- âœ… TÄƒng 1186% qua 10 epochs
- âœ… GiÃ¡ trá»‹ há»£p lÃ½ (khÃ´ng quÃ¡ lá»›n)
- ðŸŽ¯ NghÄ©a: Agent Ä‘ang há»c Ä‘Æ°á»£c cÃ¡ch chá»n actions tá»‘t hÆ¡n

### ðŸ”¹ Subplot 4: Maximum Q-Value per Epoch
**Ã nghÄ©a**: Q-value cao nháº¥t trong báº£ng

```
Tá»‘t: TÄƒng dáº§n, chá»©ng tá» cÃ³ actions ráº¥t tá»‘t
Xáº¥u: TÄƒng vá»t quÃ¡ nhanh (instability)
```

**VÃ­ dá»¥ cá»§a báº¡n**:
- Start: 32.723 â†’ End: 88.251
- âœ… TÄƒng 170% (á»•n Ä‘á»‹nh)
- ðŸŽ¯ NghÄ©a: ÄÃ£ tÃ¬m ra nhá»¯ng learning paths ráº¥t tá»‘t cho má»™t sá»‘ students

---

## 2. ðŸ“Š Q-Value Evolution (qvalue_evolution.png)

### Má»¥c Ä‘Ã­ch: So sÃ¡nh phÃ¢n phá»‘i Q-values trÆ°á»›c vÃ  sau training

**CÃ¡c Ä‘iá»ƒm cáº§n xem**:

### ðŸ”¹ Initial Q-values (Epoch 1)
```
- MÃ u xanh (blue)
- ThÆ°á»ng táº­p trung gáº§n 0
- PhÃ¢n phá»‘i háº¹p
```
**Ã nghÄ©a**: Agent chÆ°a biáº¿t gÃ¬ vá» mÃ´i trÆ°á»ng

### ðŸ”¹ Final Q-values (Epoch 10)
```
- MÃ u cam (orange)  
- PhÃ¢n phá»‘i rá»™ng hÆ¡n
- CÃ³ nhiá»u values lá»›n hÆ¡n
```
**Ã nghÄ©a**: Agent Ä‘Ã£ há»c Ä‘Æ°á»£c policies tá»‘t

### ðŸ”¹ Äiá»u cáº§n xem:
1. **Shift to the right** âœ… = Model há»c Ä‘Æ°á»£c rewards tÃ­ch cá»±c
2. **Wider spread** âœ… = Model phÃ¢n biá»‡t Ä‘Æ°á»£c actions tá»‘t/xáº¥u
3. **No extreme outliers** âœ… = Training á»•n Ä‘á»‹nh

**VÃ­ dá»¥ cá»§a báº¡n**:
- Initial: Centered around 0.4
- Final: Spread from 0 to 88
- âœ… Ráº¥t tá»‘t: Model Ä‘Ã£ há»c máº¡nh

---

## 3. ðŸ“„ Training Summary (training_summary.txt)

### ðŸ”¹ Reward Statistics
```yaml
Initial reward: 69.267    # Epoch Ä‘áº§u
Final reward: 69.267      # Epoch cuá»‘i
Max reward: 69.267        # Cao nháº¥t
Average: 69.267           # Trung bÃ¬nh
```

**PhÃ¢n tÃ­ch cá»§a báº¡n**:
- âš ï¸ KhÃ´ng Ä‘á»•i â†’ Model Ä‘Ã£ converge hoáº·c data Ä‘Ã£ trained
- âœ… Stable â†’ KhÃ´ng bá»‹ divergence

### ðŸ”¹ Q-Table Growth
```yaml
Initial: 35,366 states
Final: 35,366 states
Growth: +0 states
Growth rate: 0.0%
```

**PhÃ¢n tÃ­ch cá»§a báº¡n**:
- âœ… Q-table size khÃ´ng tÄƒng = ÄÃ£ explore háº¿t state space
- ðŸŽ¯ 35,366 states ráº¥t lá»›n = Coverage tá»‘t

### ðŸ”¹ Q-Value Statistics
```yaml
Avg Q-value: 0.413 â†’ 5.315   (+1186%)
Max Q-value: 32.723 â†’ 88.251 (+170%)
```

**PhÃ¢n tÃ­ch cá»§a báº¡n**:
- âœ…âœ…âœ… TÄƒng máº¡nh = Learning ráº¥t hiá»‡u quáº£
- âœ… KhÃ´ng cÃ³ explosion = Stable training

### ðŸ”¹ Convergence
```yaml
Last 3 epochs variance: 0.000000
Converged: Yes âœ“
```

**PhÃ¢n tÃ­ch cá»§a báº¡n**:
- âœ… Variance = 0 = Model Ä‘Ã£ converge hoÃ n toÃ n
- ðŸŽ¯ CÃ³ thá»ƒ stop training sá»›m Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian

---

## ðŸš¨ Dáº¥u Hiá»‡u Cáº§n ChÃº Ã

### âŒ BAD SIGNS:
1. **Reward giáº£m dáº§n**: Overfitting hoáº·c learning rate quÃ¡ cao
2. **Q-values bÃ¹ng ná»•** (>1000): Training khÃ´ng stable
3. **Q-table tÄƒng liÃªn tá»¥c**: State space quÃ¡ lá»›n
4. **Avg Q-value giáº£m**: Model Ä‘ang unlearn

### âœ… GOOD SIGNS (NhÆ° training cá»§a báº¡n):
1. **Reward stable**: âœ…
2. **Q-values tÄƒng á»•n Ä‘á»‹nh**: âœ… 
3. **Q-table size há»£p lÃ½**: âœ…
4. **Converged**: âœ…

---

## ðŸŽ¯ Káº¿t Luáº­n Cho Training Cá»§a Báº¡n

### Äiá»ƒm Máº¡nh:
1. âœ… **Q-table ráº¥t lá»›n**: 35,366 states (coverage tá»‘t)
2. âœ… **Q-values tÄƒng máº¡nh**: Avg +1186%, Max +170%
3. âœ… **Training stable**: KhÃ´ng cÃ³ divergence
4. âœ… **ÄÃ£ converge**: Variance = 0

### Khuyáº¿n Nghá»‹:
1. ðŸŽ¯ **Model sáºµn sÃ ng production**: Cháº¥t lÆ°á»£ng tá»‘t
2. ðŸ’¡ **CÃ³ thá»ƒ giáº£m epochs**: 10 epochs cÃ³ thá»ƒ thá»«a, test vá»›i 5 epochs
3. ðŸ” **Monitor trong production**: Xem Q-values cÃ³ phÃ¹ há»£p vá»›i real users khÃ´ng
4. ðŸ“Š **Collect more data**: Náº¿u muá»‘n improve thÃªm

### So SÃ¡nh Vá»›i Training TrÆ°á»›c:
```
                    TrÆ°á»›c      â†’    BÃ¢y giá»
States:             2,717      â†’    35,366    (+1200%)
Q-values > 0:       100%       â†’    100%      (maintained)
Avg Q-value:        3.1        â†’    5.315     (+71%)
Coverage:           5.4%       â†’    70.7%     (+1200%)
```

---

## ðŸ“š CÃ¡ch Sá»­ Dá»¥ng Biá»ƒu Äá»“

### Trong Development:
```bash
# Cháº¡y training vá»›i sá»‘ epochs khÃ¡c nhau
python3 train_qlearning_v2.py --epochs 5
python3 train_qlearning_v2.py --epochs 20

# So sÃ¡nh cÃ¡c plots Ä‘á»ƒ tÃ¬m sá»‘ epochs tá»‘i Æ°u
```

### Trong Production:
1. LÆ°u plots má»—i láº§n retrain
2. So sÃ¡nh vá»›i láº§n train trÆ°á»›c
3. Alert náº¿u Q-values giáº£m hoáº·c reward drop

### Debug Issues:
- **Reward khÃ´ng tÄƒng**: Check data quality
- **Q-table quÃ¡ lá»›n**: TÄƒng state_decimals trong config
- **Q-values = 0**: Cáº§n thÃªm exploration (tÄƒng epsilon)

---

## ðŸ”§ Tuning Hyperparameters

Dá»±a vÃ o biá»ƒu Ä‘á»“, báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh:

### Learning Rate (Î±):
- Reward dao Ä‘á»™ng â†’ Giáº£m Î±
- Há»c quÃ¡ cháº­m â†’ TÄƒng Î±
- **Cá»§a báº¡n**: 0.1 lÃ  tá»‘t âœ…

### Discount Factor (Î³):
- Q-values quÃ¡ tháº¥p â†’ TÄƒng Î³
- Q-values bÃ¹ng ná»• â†’ Giáº£m Î³  
- **Cá»§a báº¡n**: 0.95 lÃ  tá»‘t âœ…

### Exploration (Îµ):
- Q-table khÃ´ng tÄƒng â†’ TÄƒng Îµ
- Reward khÃ´ng stable â†’ Giáº£m Îµ
- **Cá»§a báº¡n**: 0.1 cÃ³ thá»ƒ tÄƒng lÃªn 0.2 Ä‘á»ƒ explore thÃªm

---

## ðŸ“ž CÃ¢u Há»i ThÆ°á»ng Gáº·p

**Q: Táº¡i sao reward khÃ´ng Ä‘á»•i?**
A: Model Ä‘Ã£ converge hoáº·c data Ä‘Ã£ Ä‘Æ°á»£c trained trÆ°á»›c. KhÃ´ng pháº£i váº¥n Ä‘á» náº¿u Q-values váº«n tÄƒng.

**Q: Bao nhiÃªu epochs lÃ  Ä‘á»§?**
A: Xem convergence trong summary.txt. Náº¿u converged = Yes, cÃ³ thá»ƒ stop.

**Q: Q-table size bao nhiÃªu lÃ  tá»‘t?**
A: Phá»¥ thuá»™c vÃ o sá»‘ students vÃ  actions. 35k states cho 200 users lÃ  ráº¥t tá»‘t.

**Q: LÃ m sao biáº¿t model Ä‘Ã£ sáºµn sÃ ng production?**
A: Check:
- âœ… Q-values > 0 
- âœ… Reward stable
- âœ… Converged = Yes
- âœ… Q-table size há»£p lÃ½

---

ðŸŽ‰ **Model cá»§a báº¡n Ä‘Ã£ Ä‘áº¡t táº¥t cáº£ tiÃªu chÃ­ â†’ Ready for production!**
