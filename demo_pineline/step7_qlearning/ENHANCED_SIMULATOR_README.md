# ğŸ“ Enhanced StudentSimulatorV2 - Quick Start

## âœ¨ TÃ­nh nÄƒng má»›i (So vá»›i báº£n cÅ©)

| Feature | Báº£n cÅ© | Báº£n má»›i (Enhanced) |
|---------|--------|-------------------|
| **Parameters** | Hardcoded by hand | âœ… Há»c tá»« `cluster_profiles.json` |
| **Progress Model** | Linear | âœ… Learning Curve (Logistic/Exponential) |
| **Quiz Tracking** | Average score only | âœ… Attempt-level vá»›i improvement history |
| **Action Selection** | Simple Îµ-greedy | âœ… Policy-based tá»« Q-table |
| **Reward** | Basic | âœ… Tuned cho RL objectives |

## ğŸš€ Quick Start

### 1. CÃ i Ä‘áº·t cÆ¡ báº£n

```python
from core.simulator_v2 import StudentSimulatorV2

# Initialize vá»›i táº¥t cáº£ features
simulator = StudentSimulatorV2(
    use_learned_params=True,      # Há»c tá»« real data
    learning_curve_type='logistic', # hoáº·c 'exponential'
    qtable_path='models/qtable.pkl',  # Optional: for policy
    seed=42
)
```

### 2. Simulate 1 há»c sinh

```python
trajectory = simulator.simulate_trajectory(
    student_id=1001,
    cluster_id=0,  # 0=weak, 1/2/4=strong, 5=medium
    max_steps=50,
    verbose=True
)

print(f"Generated {len(trajectory)} transitions")
print(f"Total reward: {sum(t['reward'] for t in trajectory):.2f}")
```

### 3. Batch simulation

```python
trajectories = simulator.simulate_batch(
    n_students_per_cluster=10,
    max_steps_per_student=100
)

# Save for training
simulator.save_trajectories(trajectories, 'data/training_data.json')
```

## ğŸ“Š Learned Parameters

Simulator tá»± Ä‘á»™ng há»c cÃ¡c parameters nÃ y tá»« `cluster_profiles.json`:

### Cluster 0 (Weak Learner)
```
âœ“ Success rate:       0.411  (41% thÃ nh cÃ´ng láº§n Ä‘áº§u)
âœ“ Stuck probability:  0.150  (15% chance bá»‹ stuck)
âœ“ Progress speed:     0.400  (tiáº¿n bá»™ cháº­m)
âœ“ Score range:        (0.26, 0.56)
âœ“ Preferred actions:  do_assignment, watch_video, do_quiz
```

### Cluster 2 (Strong Learner)
```
âœ“ Success rate:       0.854  (85% thÃ nh cÃ´ng)
âœ“ Stuck probability:  0.050  (5% bá»‹ stuck)
âœ“ Progress speed:     0.400  (tiáº¿n bá»™ nhanh)
âœ“ Score range:        (0.70, 1.00)
âœ“ Preferred actions:  do_assignment, do_quiz, watch_video
```

## ğŸ“ˆ Learning Curve Examples

### Weak Learner Progress
```
Attempt  1 â†’ Progress: 0.007  (slow start)
Attempt  3 â†’ Progress: 0.041
Attempt  5 â†’ Progress: 0.182  (picking up)
Attempt  8 â†’ Progress: 0.500  (midpoint!)
Attempt 10 â†’ Progress: 0.689
Attempt 15 â†’ Progress: 0.952  (plateau)
```

### Strong Learner Progress
```
Attempt  1 â†’ Progress: 0.310  (fast start!)
Attempt  3 â†’ Progress: 0.822  (midpoint at attempt 3)
Attempt  5 â†’ Progress: 0.975  (quickly reaches mastery)
Attempt  7 â†’ Progress: 0.997
```

## ğŸ¯ Use Cases

### 1. Generate Training Data cho Q-Learning

```python
simulator = StudentSimulatorV2(use_learned_params=True)

# Generate 100 students
trajectories = simulator.simulate_batch(
    n_students_per_cluster=20,
    max_steps_per_student=100
)

# Total: 20 * 5 clusters = 100 students
# Save for training
simulator.save_trajectories(trajectories, 'data/qtable_training_data.json')
```

### 2. Test Q-table vá»›i Realistic Behavior

```python
simulator = StudentSimulatorV2(
    qtable_path='models/trained_qtable.pkl',
    use_learned_params=True
)

# Simulate vÃ  xem Q-table cÃ³ recommend Ä‘Ãºng khÃ´ng
for cluster_id in [0, 1, 2]:
    traj = simulator.simulate_trajectory(
        student_id=1000 + cluster_id,
        cluster_id=cluster_id,
        max_steps=30
    )
    print(f"Cluster {cluster_id}: Avg reward = {np.mean([t['reward'] for t in traj]):.2f}")
```

### 3. Compare Learning Curves

```python
# Logistic curve
sim_logistic = StudentSimulatorV2(learning_curve_type='logistic')
traj_log = sim_logistic.simulate_trajectory(1, 0, max_steps=20)

# Exponential curve  
sim_exp = StudentSimulatorV2(learning_curve_type='exponential')
traj_exp = sim_exp.simulate_trajectory(2, 0, max_steps=20)

# Compare progress patterns
```

## ğŸ§ª Testing

Cháº¡y comprehensive test suite:

```bash
python3 test_enhanced_simulator.py
```

Output:
```
âœ… TEST 1: Learned Parameters âœ“
âœ… TEST 2: Learning Curve Model âœ“
âœ… TEST 3: Attempt-Level Quiz Tracking âœ“
âœ… TEST 4: Complete Trajectory âœ“
âœ… TEST 5: Policy-Based Selection âœ“
âœ… TEST 6: Batch Simulation âœ“
âœ… TEST 7: Comparison Tests âœ“

ğŸ‰ ALL TESTS PASSED!
```

## ğŸ“¦ Output Format

Má»—i transition trong trajectory:

```python
{
    'state': (0, 0, 0.25, 0.50, 2, False),  # (cluster, module_idx, progress, score, action, stuck)
    'action': 46,                            # Module ID
    'action_type': 'do_quiz',               # Human-readable action
    'reward': 2.5,                          # Reward value
    'next_state': (0, 0, 0.35, 0.52, 2, False),
    'module_progress': 0.35,                # Current module progress [0-1]
    'avg_score': 0.52,                      # Average score [0-1]
    'is_stuck': False,                      # Stuck state flag
    'is_terminal': False,                   # Episode end flag
    'completed': False,                     # Module completed flag
    'timestamp': datetime(2024, 1, 1, 9, 15)
}
```

## ğŸ”§ Advanced Configuration

### Custom Learning Curve

```python
# Modify curve parameters for specific cluster
simulator.learning_curve_params['weak']['k'] = 0.4  # Steeper learning

# Re-run simulation
trajectory = simulator.simulate_trajectory(1, 0, max_steps=20)
```

### Custom Reward Function

```python
# Simulator uses RewardCalculatorV2
# Modify trong core/reward_calculator_v2.py náº¿u cáº§n
```

### Disable Specific Features

```python
# KhÃ´ng dÃ¹ng learned params
simulator = StudentSimulatorV2(use_learned_params=False)

# KhÃ´ng dÃ¹ng learning curve (linear progress)
# â†’ Chá»‰ cáº§n khÃ´ng set learning_curve_type
```

## ğŸ“Š Comparison vá»›i Old Simulator

| Metric | Old Simulator | Enhanced Simulator |
|--------|--------------|-------------------|
| Progress pattern | Linear | Realistic curve (slow â†’ fast â†’ plateau) |
| Score improvement | Random | Learning-based improvement |
| Action selection | Fixed Îµ-greedy | Policy-based hoáº·c learned Îµ |
| Parameters | Manual tuning | Auto-learned tá»« data |
| Validation | Difficult | Match vá»›i real logs |

## ğŸ¯ Integration vá»›i Training Pipeline

```python
# 1. Generate training data
simulator = StudentSimulatorV2(use_learned_params=True, seed=42)
trajectories = simulator.simulate_batch(n_students_per_cluster=50)
simulator.save_trajectories(trajectories, 'data/training_trajectories.json')

# 2. Train Q-learning agent
from core.qlearning_agent_v2 import QLearningAgentV2
agent = QLearningAgentV2()
agent.train_from_trajectories('data/training_trajectories.json')
agent.save_qtable('models/trained_qtable.pkl')

# 3. Test vá»›i simulator using trained Q-table
test_simulator = StudentSimulatorV2(
    qtable_path='models/trained_qtable.pkl',
    use_learned_params=True
)
test_trajectories = test_simulator.simulate_batch(n_students_per_cluster=10)

# 4. Evaluate
evaluate_trajectories(test_trajectories)
```

## ğŸ“š Documentation

- **Chi tiáº¿t Ä‘áº§y Ä‘á»§**: [`ENHANCED_SIMULATOR_DOCS.md`](ENHANCED_SIMULATOR_DOCS.md)
- **API Reference**: Trong code comments
- **Examples**: `test_enhanced_simulator.py`

## â“ FAQ

### Q: Táº¡i sao learning curve quan trá»ng?
**A**: Learning curve mÃ´ phá»ng cÃ¡ch ngÆ°á»i há»c tháº­t tiáº¿n bá»™ - cháº­m lÃºc Ä‘áº§u, nhanh á»Ÿ giá»¯a, plateau cuá»‘i. Linear progress khÃ´ng realistic.

### Q: Attempt-level tracking khÃ¡c gÃ¬ vá»›i average score?
**A**: Thay vÃ¬ chá»‰ lÆ°u average, ta lÆ°u tá»«ng attempt â†’ tháº¥y Ä‘Æ°á»£c improvement pattern â†’ realistic hÆ¡n.

### Q: Khi nÃ o cáº§n Q-table?
**A**: 
- **Training**: KhÃ´ng cáº§n Q-table, dÃ¹ng heuristic
- **Testing/Production**: Cáº§n Q-table Ä‘á»ƒ test policy learned

### Q: Learned params cÃ³ chÃ­nh xÃ¡c khÃ´ng?
**A**: TÃ¹y quality cá»§a `cluster_profiles.json`. Náº¿u data tá»‘t â†’ params accurate. Test báº±ng cÃ¡ch compare vá»›i real logs.

### Q: CÃ³ thá»ƒ tune parameters khÃ´ng?
**A**: CÃ³! Set `use_learned_params=False` vÃ  modify trong `_initialize_cluster_params()`.

## ğŸš¨ Common Issues

### Issue: Q-table khÃ´ng load Ä‘Æ°á»£c
```
âš  Q-table khÃ´ng tÃ¬m tháº¥y táº¡i: models/qtable.pkl
```
**Solution**: Train Q-table trÆ°á»›c hoáº·c bá» `qtable_path` parameter.

### Issue: Learned params khÃ´ng há»£p lÃ½
```
Cluster 0: success=0.99  # Too high for weak learner
```
**Solution**: Kiá»ƒm tra `cluster_profiles.json` cÃ³ Ä‘Ãºng format khÃ´ng.

### Issue: Progress quÃ¡ nhanh/cháº­m
```
Progress sau 5 attempts: 0.99  # Too fast
```
**Solution**: Adjust learning curve parameters trong `_initialize_learning_curves()`.

## ğŸ‰ Success Metrics

Sau khi implement, kiá»ƒm tra:

- âœ… Learned params reasonable cho má»—i cluster
- âœ… Learning curve smooth vÃ  realistic
- âœ… Scores improve qua attempts
- âœ… Action distribution match vá»›i cluster level
- âœ… Trajectories cÃ³ thá»ƒ train Q-learning thÃ nh cÃ´ng

## ğŸ”— Next Steps

1. Generate large training dataset (1000+ students)
2. Train Q-learning vá»›i dataset nÃ y
3. Evaluate trained Q-table vá»›i test simulator
4. Deploy recommendations to production
5. A/B test vá»›i real students

---

**Version**: 2.0 Enhanced  
**Status**: âœ… Production Ready  
**Last Updated**: 2024-11-06
