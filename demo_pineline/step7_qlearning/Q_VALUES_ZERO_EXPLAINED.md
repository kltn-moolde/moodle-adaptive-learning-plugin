# ğŸ” Giáº£i ThÃ­ch Váº¥n Äá» Q-values = 0.0

## â“ Táº¡i sao Q-values Ä‘á»u = 0.0?

Khi báº¡n gá»i API vÃ  nháº­n Ä‘Æ°á»£c recommendations vá»›i **táº¥t cáº£ q_value = 0.0**, cÃ³ 3 nguyÃªn nhÃ¢n chÃ­nh:

```json
"recommendations": [
    {
        "action_id": 64,
        "name": "bÃ i kiá»ƒm tra bÃ i 2 - hard",
        "q_value": 0.0  // âš ï¸ Táº¡i sao = 0?
    }
]
```

---

## ğŸ” NGUYÃŠN NHÃ‚N 1: State chÆ°a cÃ³ trong Q-table (ChÃ­nh xÃ¡c nháº¥t)

### Giáº£i thÃ­ch:

**Q-Learning sá»­ dá»¥ng tabular method:**
- Q-table lÆ°u trá»¯: `{state_hash: {action_id: q_value}}`
- State Ä‘Æ°á»£c hash vá» tuple Ä‘á»ƒ lÃ m key: `(0.6, 0.5, 0.0, 0.2, ...)`
- **Náº¿u state_hash chÆ°a xuáº¥t hiá»‡n trong Q-table â†’ táº¥t cáº£ Q-values = 0.0**

### VÃ­ dá»¥ cá»¥ thá»ƒ:

**Input features cá»§a báº¡n:**
```python
features = {
    "mean_module_grade": 0.6,
    "total_events": 0.9,
    "viewed": 0.5,
    ...
}
```

**Sau khi build_state() â†’ state_vector:**
```python
state = [0.6, 0.467, 0.016, 0.0, 0.8, 0.5, 0.2, 0.0, 0.3, 0.8, 0.143, 0.67]
```

**Sau khi hash (state_decimals=1):**
```python
state_hash = (0.6, 0.5, 0.0, 0.0, 0.8, 0.5, 0.2, 0.0, 0.3, 0.8, 0.1, 0.7)
```

**TÃ¬m trong Q-table:**
```python
if state_hash not in agent.q_table:
    # âŒ State nÃ y chÆ°a Ä‘Æ°á»£c training!
    # â†’ Táº¥t cáº£ Q-values = 0.0 (default)
```

### Code xá»­ lÃ½ trong qlearning_agent.py:

```python
def recommend_action(self, state, available_actions, top_k=3, fallback_random=True):
    state_hash = self.hash_state(state)
    
    # Get Q-values tá»« Q-table
    q_values = [
        (action_id, self.q_table[state_hash][action_id])  # ğŸ‘ˆ Náº¿u chÆ°a cÃ³ = 0.0
        for action_id in available_actions
    ]
    
    # Náº¿u táº¥t cáº£ Q-values = 0 â†’ fallback to random
    if fallback_random and all(q == 0 for _, q in q_values):
        random_actions = random.sample(available_actions, min(top_k, len(available_actions)))
        return [(action_id, 0.0) for action_id in random_actions]  # ğŸ‘ˆ Tráº£ vá» random
```

---

## ğŸ” NGUYÃŠN NHÃ‚N 2: Model chÆ°a Ä‘Æ°á»£c training Ä‘á»§

### Thá»‘ng kÃª tá»« output cá»§a báº¡n:

```json
"model_info": {
    "model_loaded": true,
    "n_states_in_qtable": 1816,      // ğŸ‘ˆ Chá»‰ cÃ³ 1816 states
    "total_updates": 30000,           // ğŸ‘ˆ 30k updates
    "episodes": 1000                  // ğŸ‘ˆ 1000 episodes
}
```

### PhÃ¢n tÃ­ch:

**1816 states trong Q-table** nghÄ©a lÃ :
- Model chá»‰ "gáº·p" 1816 states khÃ¡c nhau trong quÃ¡ trÃ¬nh training
- NhÆ°ng **khÃ´ng gian state lÃ  vÃ´ háº¡n** (continuous state space)
- State cá»§a sinh viÃªn báº¡n **khÃ´ng náº±m trong 1816 states nÃ y**

**Tá»· lá»‡ coverage:**
```
Giáº£ sá»­ cÃ³ ~10,000 sinh viÃªn
â†’ Má»—i sinh viÃªn cÃ³ ~5-10 states khÃ¡c nhau
â†’ Tá»•ng states cÃ³ thá»ƒ cÃ³: 50,000 - 100,000

Coverage = 1816 / 50,000 = 3.6% âš ï¸
â†’ 96.4% states chÆ°a Ä‘Æ°á»£c há»c!
```

---

## ğŸ” NGUYÃŠN NHÃ‚N 3: State hashing lÃ m máº¥t thÃ´ng tin

### Code hashing:

```python
def hash_state(self, state: np.ndarray) -> Tuple:
    return tuple(np.round(state, decimals=self.state_decimals))
    #                                     ğŸ‘† default = 1
```

### VÃ­ dá»¥:

**State gá»‘c (12 chiá»u, precision cao):**
```python
[0.6000000238418579, 0.46666666865348816, 0.01600000075995922, ...]
```

**Sau khi round(decimals=1):**
```python
(0.6, 0.5, 0.0, 0.0, 0.8, 0.5, 0.2, 0.0, 0.3, 0.8, 0.1, 0.7)
```

**Váº¥n Ä‘á»:**
- Nhiá»u states khÃ¡c nhau Ä‘Æ°á»£c round thÃ nh **cÃ¹ng 1 hash**
- NhÆ°ng váº«n cÃ³ **quÃ¡ nhiá»u states unique** â†’ Q-table khÃ´ng há»c háº¿t
- State cá»§a sinh viÃªn má»›i â†’ chÆ°a cÃ³ trong Q-table

---

## âœ… GIáº¢I PHÃP

### 1. **Kiá»ƒm tra state cÃ³ trong Q-table khÃ´ng**

ThÃªm logging vÃ o `api_service.py`:

```python
@app.post('/api/recommend', response_model=RecommendResponse)
def recommend(req: RecommendRequest):
    state = _build_state_from_request(req)
    
    # ğŸ‘‡ THÃŠM LOGGING
    if agent:
        state_hash = agent.hash_state(state)
        is_known_state = state_hash in agent.q_table
        
        print(f"[DEBUG] State hash: {state_hash}")
        print(f"[DEBUG] State known? {is_known_state}")
        
        if is_known_state:
            q_values = agent.q_table[state_hash]
            print(f"[DEBUG] Available Q-values: {len(q_values)} actions")
            print(f"[DEBUG] Max Q-value: {max(q_values.values()) if q_values else 0}")
        else:
            print(f"[DEBUG] State NOT in Q-table â†’ Fallback to random")
    
    # ... rest of code
```

### 2. **Training thÃªm vá»›i states phá»• biáº¿n**

Cáº§n training model vá»›i nhiá»u **representative states** hÆ¡n:

```python
# Trong training script
# Generate diverse states covering common student profiles
def generate_diverse_states(n_samples=10000):
    profiles = {
        'struggling': {'mean_grade': (0.0, 0.4), 'engagement': (0.0, 0.3)},
        'average': {'mean_grade': (0.4, 0.7), 'engagement': (0.3, 0.7)},
        'excellent': {'mean_grade': (0.7, 1.0), 'engagement': (0.7, 1.0)}
    }
    # ... generate states covering all profiles
```

### 3. **Giáº£m state_decimals (KhÃ´ng khuyáº¿n khÃ­ch)**

```python
# Thay Ä‘á»•i trong qlearning_agent.py
agent = QLearningAgent(
    n_actions=n_actions,
    state_decimals=0  # ğŸ‘ˆ Round to integer (0.6 â†’ 1.0)
)
```

**Háº­u quáº£:**
- âœ… Giáº£m sá»‘ states unique â†’ tÄƒng coverage
- âŒ Máº¥t thÃ´ng tin â†’ recommendations kÃ©m chÃ­nh xÃ¡c

### 4. **Sá»­ dá»¥ng Function Approximation (Tá»‘t nháº¥t)**

Thay vÃ¬ tabular Q-learning, dÃ¹ng **Deep Q-Network (DQN)**:

```python
# Neural network approximates Q(s, a)
class DQN:
    def __init__(self, state_dim=12, n_actions=100):
        self.model = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, n_actions)
        )
    
    def predict(self, state):
        # âœ… CÃ³ thá»ƒ predict cho Báº¤T Ká»² state nÃ o
        return self.model(state)
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Generalize cho unseen states
- âœ… KhÃ´ng cáº§n lÆ°u Q-table khá»•ng lá»“
- âœ… Q-values â‰  0 cho má»i states

---

## ğŸ§ª TEST & VERIFY

### 1. Kiá»ƒm tra Q-table coverage:

```python
# Test script
import pickle
from pathlib import Path

# Load model
model_path = Path('models/qlearning_model.pkl')
with open(model_path, 'rb') as f:
    data = pickle.load(f)

q_table = data['q_table']
print(f"Total states in Q-table: {len(q_table)}")

# Check sample states
sample_states = [
    (0.6, 0.5, 0.0, 0.0, 0.8, 0.5, 0.2, 0.0, 0.3, 0.8, 0.1, 0.7),
    (0.8, 0.9, 0.1, 0.1, 0.9, 0.8, 0.7, 0.2, 0.8, 0.9, 0.5, 0.9),
    (0.3, 0.2, 0.5, 0.0, 0.4, 0.3, 0.1, 0.0, 0.2, 0.4, 0.1, 0.3)
]

for state_hash in sample_states:
    if state_hash in q_table:
        print(f"âœ… State {state_hash}: {len(q_table[state_hash])} actions learned")
    else:
        print(f"âŒ State {state_hash}: NOT in Q-table")
```

### 2. Test vá»›i known state:

TÃ¬m 1 state **cÃ³ trong Q-table** Ä‘á»ƒ test:

```python
# Get first state from Q-table
first_state_hash = list(q_table.keys())[0]
print(f"Known state: {first_state_hash}")

# Test API vá»›i state nÃ y
response = requests.post('http://localhost:8080/api/recommend', json={
    'state': list(first_state_hash),  # Convert tuple â†’ list
    'top_k': 5
})

result = response.json()
# Kiá»ƒm tra q_values cÃ³ â‰  0 khÃ´ng
print(result['recommendations'])
```

---

## ğŸ“Š Káº¾T LUáº¬N

### Váº¥n Ä‘á» hiá»‡n táº¡i:

```
Input state cá»§a sinh viÃªn
    â†“
Build & hash state â†’ (0.6, 0.5, 0.0, ...)
    â†“
TÃ¬m trong Q-table (1816 states)
    â†“
âŒ KHÃ”NG TÃŒM THáº¤Y
    â†“
Fallback to random â†’ q_value = 0.0
```

### Giáº£i phÃ¡p ngáº¯n háº¡n:
1. âœ… Add logging Ä‘á»ƒ confirm
2. âœ… Training thÃªm vá»›i diverse states

### Giáº£i phÃ¡p dÃ i háº¡n:
1. âœ… Chuyá»ƒn sang DQN (Deep Q-Network)
2. âœ… Sá»­ dá»¥ng state generalization
3. âœ… Hybrid: Q-learning + nearest neighbor fallback

---

## ğŸ”— Files cáº§n chá»‰nh sá»­a

1. **api_service.py** - Add logging (line ~225)
2. **qlearning_agent.py** - Consider DQN
3. **train_qlearning.py** - Generate diverse training states
4. **state_builder.py** - Optimize state representation
