# ğŸ“ Q-LEARNING ADAPTIVE LEARNING - HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG

## ğŸ“‹ Má»¤C Lá»¤C

1. [Giá»›i thiá»‡u](#giá»›i-thiá»‡u)
2. [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
3. [Kiáº¿n trÃºc há»‡ thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
4. [Thiáº¿t káº¿ Q-Table](#thiáº¿t-káº¿-q-table)
5. [Sá»­ dá»¥ng cÆ¡ báº£n](#sá»­-dá»¥ng-cÆ¡-báº£n)
6. [TÃ¹y chá»‰nh vÃ  má»Ÿ rá»™ng](#tÃ¹y-chá»‰nh-vÃ -má»Ÿ-rá»™ng)
7. [Best Practices](#best-practices)

---

## ğŸ¯ GIá»šI THIá»†U

### Há»‡ thá»‘ng lÃ m gÃ¬?

Gá»£i Ã½ lá»™ trÃ¬nh há»c táº­p **Tá»I Æ¯U** cho tá»«ng sinh viÃªn dá»±a trÃªn:
- âœ… Profile hiá»‡n táº¡i (grades, engagement, consistency)
- âœ… HÃ nh vi há»c táº­p trÆ°á»›c Ä‘Ã³
- âœ… Cáº¥u trÃºc khÃ³a há»c (prerequisites, difficulty)
- âœ… Má»¥c tiÃªu: Maximize learning outcomes

### Táº¡i sao dÃ¹ng Q-Learning?

| **TiÃªu chÃ­** | **Rule-based** | **ML Classifier** | **Q-Learning** âœ… |
|--------------|----------------|-------------------|-------------------|
| Personalization | âŒ Tháº¥p | âš ï¸ Trung bÃ¬nh | âœ… **Cao** |
| Adaptability | âŒ Cá»©ng nháº¯c | âš ï¸ Cáº§n retrain | âœ… **LiÃªn tá»¥c há»c** |
| Explainability | âœ… RÃµ rÃ ng | âŒ Black-box | âš ï¸ Q-values |
| Data requirement | âœ… Ãt | âŒ Nhiá»u | âš ï¸ Trung bÃ¬nh |

---

## ğŸ”§ CÃ€I Äáº¶T

### 1. Prerequisites

```bash
Python >= 3.8
```

### 2. Install dependencies

```bash
cd step7_qlearning
pip install -r requirements.txt
```

### 3. Quick test

```bash
python examples/quick_demo.py
```

**Output mong Ä‘á»£i:**
```
ğŸ“ Q-LEARNING ADAPTIVE LEARNING SYSTEM
   Demo: Course Recommendation

============================================================
DEMO: Q-LEARNING TRAINING
============================================================

1. Creating course structure...
   âœ“ Course: Demo Course
   âœ“ Modules: 2
   âœ“ Activities: 5

2. Creating Q-Learning agent...
   âœ“ Agent created: QLearningAgent(...)
   âœ“ State dimension: 21

...
âœ… Demo completed!
```

---

## ğŸ—ï¸ KIáº¾N TRÃšC Há»† THá»NG

### Components Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CourseStructure                        â”‚
â”‚  - Modules, Activities                                   â”‚
â”‚  - Prerequisite graph                                    â”‚
â”‚  - Course metadata                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  StudentProfile                          â”‚
â”‚  - Learning history                                      â”‚
â”‚  - Grades, completion                                    â”‚
â”‚  - Derived features (engagement, consistency)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               AbstractStateBuilder                       â”‚
â”‚  Student features + Activity context â†’ State vector      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  QLearningAgent                          â”‚
â”‚  Q(state, action) â†’ Expected reward                      â”‚
â”‚  Policy: Choose best action to maximize outcome          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Recommendation                           â”‚
â”‚  "You should learn: Activity X" (Q-value: 0.85)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dependency Injection

Táº¥t cáº£ components cÃ³ thá»ƒ **swap** Ä‘á»ƒ customize:

```python
# Default
agent = QLearningAgent(course_structure)

# Custom
agent = QLearningAgent(
    course_structure=course,
    state_builder=MyCustomStateBuilder(course),
    action_space=MyCustomActionSpace(course),
    reward_calculator=MyCustomRewardCalculator(course)
)
```

---

## ğŸ¯ THIáº¾T Káº¾ Q-TABLE

### KEY DESIGN: Abstract State Representation

#### âŒ BAD: Hard-coded state

```python
# KHÃ”NG lÃ m tháº¿ nÃ y!
state = {
    'completed_act_1_1': True,
    'completed_act_1_2': False,
    'grade_act_1_1': 0.85,
    ...
}
# â†’ Chá»‰ hoáº¡t Ä‘á»™ng vá»›i 1 khÃ³a há»c cá»¥ thá»ƒ
# â†’ KhÃ´ng scale Ä‘Æ°á»£c
```

#### âœ… GOOD: Abstract features

```python
# LÃ m tháº¿ nÃ y!
state = [
    0.75,  # avg_grade (normalized)
    0.40,  # completion_rate
    0.80,  # engagement_score
    0.65,  # consistency_score
    0.45,  # difficulty cá»§a activity tiáº¿p theo
    ...
]
# â†’ Hoáº¡t Ä‘á»™ng vá»›i Má»ŒI khÃ³a há»c
# â†’ Scale tá»‘t
```

### State Structure

#### **Part 1: Student Profile Features (6 features)**

```python
[
    avg_grade,           # 0-1, Ä‘iá»ƒm TB
    completion_rate,     # 0-1, % hoÃ n thÃ nh
    engagement_score,    # 0-1, má»©c Ä‘á»™ tÃ­ch cá»±c
    consistency_score,   # 0-1, Ä‘á»™ Ä‘á»u Ä‘áº·n
    progress,            # 0-1, normalized completed count
    time_since_last,     # 0-1, days since last activity (normalized)
]
```

#### **Part 2: Activity Context Features (11+ features)**

```python
[
    difficulty,              # 0-1, Ä‘á»™ khÃ³
    estimated_time,          # 0-1, normalized
    prerequisite_met,        # 0/1, binary
    n_prerequisites,         # 0-1, normalized
    is_optional,             # 0/1, binary
    type_video,              # 0/1, one-hot
    type_quiz,               # 0/1, one-hot
    type_assignment,         # 0/1, one-hot
    ...,                     # other types
    module_position,         # 0-1, vá»‹ trÃ­ trong course
    activity_depth,          # 0-1, Ä‘á»™ sÃ¢u trong prerequisite graph
    similar_success_rate,    # 0-1, success vá»›i activities tÆ°Æ¡ng tá»±
]
```

#### **Part 3: Optional - Cluster (2 features)**

```python
[
    cluster_0,  # 0/1, thuá»™c cluster_0 (good students)
    cluster_1,  # 0/1, thuá»™c cluster_1 (struggling students)
]
```

**Total: ~19-21 features**

### Q-Table Structure

```python
Q-table = Dict[(state_hash, action_id)] = Q-value

# Example:
Q[
    (0.75, 0.40, 0.80, ...),  # State hash (rounded)
    'act_2_3'                 # Action ID
] = 0.82  # Q-value (expected reward)
```

### Why this works across courses?

âœ… **Features are generic**
- KhÃ´ng cÃ³ tÃªn activity cá»¥ thá»ƒ
- Chá»‰ dÃ¹ng: difficulty, type, position, ...

âœ… **Relative, not absolute**
- Completion rate (%), khÃ´ng pháº£i count
- Module position (%), khÃ´ng pháº£i module number

âœ… **Transfer learning**
- Q-values há»c Ä‘Æ°á»£c patterns chung
- VD: "Náº¿u SV yáº¿u (grade<0.5) + activity khÃ³ (diff>0.7) â†’ Low reward"
- Pattern nÃ y Ä‘Ãºng cho Má»ŒI khÃ³a há»c!

---

## ğŸ“š Sá»¬ Dá»¤NG CÆ  Báº¢N

### 1. Load Course Structure

#### Tá»« JSON

```python
from data.course_loader import CourseLoader

course = CourseLoader.from_json('examples/course_structure_example.json')
```

#### Tá»« Database

```python
import psycopg2

conn = psycopg2.connect(
    host='localhost',
    database='moodle',
    user='user',
    password='pass'
)

course = CourseLoader.from_database(conn, course_id=123)
```

### 2. Create Agent

```python
from core.qlearning_agent import QLearningAgent

agent = QLearningAgent(
    course_structure=course,
    learning_rate=0.1,      # Alpha
    discount_factor=0.95,   # Gamma
    epsilon=0.1             # Exploration rate
)
```

### 3. Training tá»« Historical Data

```python
from training.trainer import QLearningTrainer

trainer = QLearningTrainer(agent, course)

# Train tá»« database logs
trainer.train_from_logs(
    db_connection=conn,
    userids=[8609, 8670, 9043, ...],
    n_epochs=50
)

# Save model
agent.save('models/qlearning_model.pkl')
```

### 4. Inference (Recommendation)

```python
# Load student profile
from models.student_profile import StudentProfile

student = StudentProfile(
    student_id='student_999',
    course_id=course.course_id
)

# Add learning history
from models.student_profile import LearningHistory

student.add_activity_history(LearningHistory(
    activity_id='act_1_1',
    completed=True,
    grade=None,
    time_spent_minutes=18
))

# Get recommendation
recommendations = agent.recommend(student, top_k=3)

for rec in recommendations:
    print(f"{rec['activity_name']}: Q={rec['q_value']:.3f}")
```

**Output:**
```
Variables Quiz: Q=0.820
Practice Assignment: Q=0.785
Type Conversion Lab: Q=0.692
```

---

## ğŸ”§ TÃ™Y CHá»ˆNH VÃ€ Má» Rá»˜NG

### 1. Custom State Builder

```python
from core.state_builder import AbstractStateBuilder
import numpy as np

class MyStateBuilder(AbstractStateBuilder):
    """Custom state vá»›i thÃªm features"""
    
    def build_state(self, student_profile, target_activity_id=None, 
                    current_timestamp=None):
        # Call parent
        base_state = super().build_state(
            student_profile, target_activity_id, current_timestamp
        )
        
        # Add custom features
        custom_features = [
            self._get_learning_pace(student_profile),
            self._get_quiz_performance(student_profile),
            # ... more features
        ]
        
        return np.concatenate([base_state, custom_features])
    
    def _get_learning_pace(self, student_profile):
        """TÃ­nh tá»‘c Ä‘á»™ há»c (activities/week)"""
        # Implementation
        return 0.5
    
    def get_state_dimension(self):
        return super().get_state_dimension() + 2  # +2 custom features
```

### 2. Custom Reward Function

```python
from core.reward_calculator import RewardCalculator

class MyRewardCalculator(RewardCalculator):
    """Custom reward vá»›i business rules riÃªng"""
    
    def calculate_reward(self, student_profile, action_id, outcome):
        reward = 0.0
        
        # Custom rule 1: Bonus for completing on time
        activity = self.course_structure.activities[action_id]
        if outcome.time_spent_minutes <= activity.estimated_minutes * 1.1:
            reward += 0.2
        
        # Custom rule 2: Penalty for skipping optional activities
        if activity.is_optional and not outcome.completed:
            reward -= 0.1
        
        # ... more rules
        
        return np.clip(reward, -1.0, 1.0)
```

### 3. Plugin Architecture

```python
# Sá»­ dá»¥ng custom components
agent = QLearningAgent(
    course_structure=course,
    state_builder=MyStateBuilder(course),
    reward_calculator=MyRewardCalculator(course)
)
```

---

## ğŸ’¡ BEST PRACTICES

### 1. Training

âœ… **DO:**
- Split data: 80% train, 20% validation
- Train vá»›i nhiá»u epochs (50-100)
- Monitor reward progression
- Decay epsilon theo thá»i gian (exploration â†’ exploitation)

âŒ **DON'T:**
- Train trÃªn Ã­t sinh viÃªn (<20)
- Overfit báº±ng cÃ¡ch train quÃ¡ nhiá»u epochs
- Ignore terminal states

### 2. State Design

âœ… **DO:**
- Normalize táº¥t cáº£ features vá» [0, 1]
- DÃ¹ng relative values (%, ratios)
- Include temporal features (time_since_last)

âŒ **DON'T:**
- Hard-code activity IDs
- Use absolute values (counts without normalization)
- Create too many features (>30) â†’ sparse Q-table

### 3. Reward Shaping

âœ… **DO:**
- Balance positive vÃ  negative rewards
- Clip rewards vá» [-1, 1]
- Include domain knowledge

âŒ **DON'T:**
- Give rewards quÃ¡ lá»›n (>10) â†’ unstable
- Ignore intermediate outcomes
- Reward chá»‰ dá»±a trÃªn final grade

### 4. Deployment

âœ… **DO:**
- Load model 1 láº§n, cache trong memory
- Validate prerequisites trÆ°á»›c khi recommend
- Log all recommendations for analysis

âŒ **DON'T:**
- Reload model má»—i request
- Recommend activities vá»›i prerequisites chÆ°a hoÃ n thÃ nh
- Deploy without A/B testing

---

## ğŸ“Š VALIDATION

### Metrics to track

```python
# 1. Q-value statistics
stats = agent.get_statistics()
print(f"Mean Q-value: {stats['q_value_stats']['mean']:.3f}")

# 2. Recommendation diversity
recommendations = [agent.recommend(student) for student in test_students]
unique_actions = len(set(r[0]['activity_id'] for r in recommendations))

# 3. Success rate (náº¿u cÃ³ ground truth)
correct = sum(
    1 for r, gt in zip(recommendations, ground_truth)
    if r[0]['activity_id'] == gt
)
accuracy = correct / len(ground_truth)
```

---

## ğŸš€ NEXT STEPS

1. âœ… Train model trÃªn dá»¯ liá»‡u tháº­t (15 sinh viÃªn)
2. âœ… Load `cluster_full_statistics.json` lÃ m benchmarks
3. âœ… Integrate vá»›i web service (REST API)
4. âœ… A/B testing: Q-Learning vs Random vs Rule-based
5. âœ… Monitor performance vÃ  retrain Ä‘á»‹nh ká»³

---

## ğŸ“ SUPPORT

CÃ³ váº¥n Ä‘á»? Check:
- `README.md` - Overview
- `examples/quick_demo.py` - Demo code
- `examples/course_structure_example.json` - Schema
- Unit tests trong `tests/`

**Happy Learning! ğŸ“**
