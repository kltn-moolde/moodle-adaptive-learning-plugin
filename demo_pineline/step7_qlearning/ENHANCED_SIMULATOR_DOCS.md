# Enhanced StudentSimulatorV2 - Documentation

## ðŸŽ¯ Tá»•ng quan

StudentSimulatorV2 Ä‘Ã£ Ä‘Æ°á»£c nÃ¢ng cáº¥p vá»›i cÃ¡c tÃ­nh nÄƒng má»›i Ä‘á»ƒ mÃ´ phá»ng chÃ­nh xÃ¡c hÃ nh vi há»c sinh thá»±c táº¿:

### âœ¨ TÃ­nh nÄƒng má»›i

1. **Há»c tham sá»‘ tá»« dá»¯ liá»‡u tháº­t** (`cluster_profiles.json`)
   - Success rate, progress speed, stuck probability
   - Action exploration entropy
   - Preferred actions distribution
   - Score ranges dá»±a trÃªn mean grades

2. **Learning Curve Model** (Logistic/Exponential)
   - Progress khÃ´ng cÃ²n tuyáº¿n tÃ­nh
   - TÄƒng nhanh ban Ä‘áº§u, sau Ä‘Ã³ plateau
   - Cluster-specific curve parameters
   - Realistic learning patterns

3. **Attempt-Level Quiz Tracking**
   - LÆ°u lá»‹ch sá»­ tá»«ng attempt per module
   - Score improvement qua cÃ¡c attempts
   - Learning curve Ã¡p dá»¥ng cho scores
   - Realistic retry patterns

4. **Policy-Based Action Selection**
   - Sá»­ dá»¥ng Q-table Ä‘á»ƒ select actions
   - Îµ-greedy policy vá»›i learned Q-values
   - Fallback to heuristic náº¿u khÃ´ng cÃ³ Q-table
   - Match vá»›i RL objectives

5. **Reward Tuning**
   - Realistic reward calculation
   - Match vá»›i training objectives
   - Cluster-specific reward patterns

## ðŸ“Š CÃ¡ch sá»­ dá»¥ng

### 1. Basic Usage - Learned Parameters

```python
from core.simulator_v2 import StudentSimulatorV2

# Initialize vá»›i learned parameters
simulator = StudentSimulatorV2(
    course_structure_path='data/course_structure.json',
    cluster_profiles_path='data/cluster_profiles.json',
    use_learned_params=True,  # Há»c tá»« data tháº­t
    learning_curve_type='logistic',  # hoáº·c 'exponential'
    seed=42
)

# Simulate single student
trajectory = simulator.simulate_trajectory(
    student_id=1001,
    cluster_id=0,  # Weak learner
    max_steps=50,
    verbose=True
)
```

### 2. With Q-table Policy

```python
# Initialize vá»›i Q-table for policy-based selection
simulator = StudentSimulatorV2(
    qtable_path='models/test_agent_stats.pkl',  # Path to Q-table
    use_learned_params=True,
    learning_curve_type='logistic'
)

# Actions sáº½ Ä‘Æ°á»£c chá»n dá»±a trÃªn Q-values
trajectory = simulator.simulate_trajectory(
    student_id=1002,
    cluster_id=2,  # Strong learner
    max_steps=50
)
```

### 3. Batch Simulation

```python
# Simulate nhiá»u students
trajectories = simulator.simulate_batch(
    n_students_per_cluster=10,
    max_steps_per_student=100,
    verbose=True
)

# Save for training
simulator.save_trajectories(trajectories, 'data/training_data.json')
```

## ðŸ”¬ Learning Curve Details

### Logistic Curve
```
Progress(n) = L / (1 + exp(-k * (n - x0)))
```
- **L**: Maximum value (1.0)
- **k**: Steepness (0.3 weak, 0.5 medium, 0.8 strong)
- **x0**: Midpoint attempts (8 weak, 5 medium, 3 strong)

### Exponential Curve
```
Progress(n) = a * (1 - exp(-b * n))
```
- **a**: Asymptote (0.85 weak, 0.92 medium, 0.97 strong)
- **b**: Growth rate (0.12 weak, 0.15 medium, 0.20 strong)

## ðŸ“ˆ Learned Parameters tá»« cluster_profiles.json

### Extraction Process

1. **Success Rate**: `mean_module_grade / 100`
2. **Stuck Probability**: `quiz_reviewed / quiz_submitted * 0.5`
3. **Progress Speed**: `0.5 / (1 + total_events / 100)`
4. **Action Exploration**: Shannon entropy cá»§a event distribution
5. **Preferred Actions**: Top 3 actions theo frequency

### Cluster Classification

```python
if success_rate > 0.75 and stuck_prob < 0.15:
    level = 'strong'
elif success_rate > 0.6 and stuck_prob < 0.25:
    level = 'medium'
else:
    level = 'weak'
```

## ðŸŽ® Attempt-Level Tracking

Má»—i module cÃ³ tracking dict:
```python
module_attempts = {
    'attempts': 5,              # Sá»‘ láº§n attempt
    'scores': [0.4, 0.5, 0.6, 0.7, 0.75],  # Lá»‹ch sá»­ scores
    'last_score': 0.75          # Score gáº§n nháº¥t
}
```

Score improvement:
```python
new_score = previous_score + (max_score - previous_score) * mastery * random(0.3, 0.7)
```

## ðŸ¤– Policy-Based Action Selection

### Vá»›i Q-table:
```python
if random() < epsilon:
    # Explore: random action
    action = random_action()
else:
    # Exploit: best Q-value
    action = argmax(Q[state, :])
```

### KhÃ´ng cÃ³ Q-table:
- Fallback to heuristic Îµ-greedy
- Progress-based action selection
- Cluster-specific preferences

## ðŸ“Š Output Format

Má»—i transition cÃ³:
```python
{
    'state': (cluster, module_idx, progress, score, action, stuck),
    'action': module_id,
    'action_type': 'do_quiz',
    'reward': 2.5,
    'next_state': (...),
    'module_progress': 0.75,
    'avg_score': 0.68,
    'is_stuck': False,
    'is_terminal': False,
    'completed': True
}
```

## ðŸ§ª Testing

Cháº¡y comprehensive test:
```bash
python3 test_enhanced_simulator.py
```

Tests bao gá»“m:
1. âœ“ Learned parameters extraction
2. âœ“ Learning curve computation
3. âœ“ Attempt-level tracking
4. âœ“ Full trajectory simulation
5. âœ“ Policy-based selection (if Q-table available)
6. âœ“ Batch simulation
7. âœ“ Comparison with/without features

## ðŸ“ Example Results

### Learned Parameters (Cluster 0 - Weak)
```
Success rate:        0.411
Stuck probability:   0.150
Progress speed:      0.400
Completion rate:     0.726
Action exploration:  0.332
Score range:         (0.261, 0.561)
Preferred actions:   do_assignment, watch_video, do_quiz
```

### Learning Curve Progress
```
WEAK Learner:
  Attempt | Expected Progress | Increment
  --------|-------------------|----------
     1    |      0.007        |   0.007
     2    |      0.017        |   0.010
     3    |      0.041        |   0.024
     5    |      0.182        |   0.088
     8    |      0.500        |   0.144  (midpoint!)
    10    |      0.689        |   0.094
    15    |      0.952        |   0.061  (plateau)
```

### Trajectory Sample
```
Step  1: watch_video     â†’ Progress: 0.015, Score: 0.500, Reward: +0.00
Step  2: read_resource   â†’ Progress: 0.035, Score: 0.500, Reward: +0.00
Step  3: do_quiz         â†’ Progress: 0.122, Score: 0.421, Reward: +0.00
Step  4: do_quiz         â†’ Progress: 0.258, Score: 0.485, Reward: +3.26  (improved!)
Step  5: do_assignment   â†’ Progress: 0.437, Score: 0.531, Reward: +3.18
```

## ðŸŽ¯ Use Cases

### 1. Training Data Generation
```python
# Generate realistic training data
simulator = StudentSimulatorV2(use_learned_params=True)
trajectories = simulator.simulate_batch(n_students_per_cluster=50)
simulator.save_trajectories(trajectories, 'data/training_data.json')
```

### 2. Q-table Validation
```python
# Test Q-table vá»›i realistic student behavior
simulator = StudentSimulatorV2(
    qtable_path='models/qtable.pkl',
    use_learned_params=True
)
trajectory = simulator.simulate_trajectory(student_id=1, cluster_id=0)
# Kiá»ƒm tra xem Q-table cÃ³ recommend Ä‘Ãºng actions khÃ´ng
```

### 3. A/B Testing
```python
# So sÃ¡nh different reward functions
sim_v1 = StudentSimulatorV2(reward_version='v1')
sim_v2 = StudentSimulatorV2(reward_version='v2')

traj_v1 = sim_v1.simulate_trajectory(1, 0)
traj_v2 = sim_v2.simulate_trajectory(1, 0)

# Compare outcomes
```

## ðŸ”§ Configuration

### Cluster Parameters
- Tá»± Ä‘á»™ng há»c tá»« `cluster_profiles.json`
- Hoáº·c dÃ¹ng manual parameters (`use_learned_params=False`)

### Learning Curve
- `logistic`: Smooth S-curve (recommended)
- `exponential`: Fast start, gradual slowdown

### Q-table Policy
- Optional: provide `qtable_path`
- Automatically falls back to heuristic

## âš ï¸ Notes

1. **Q-table format**: Pháº£i lÃ  pickle file vá»›i dict `{state: {action: q_value}}`
2. **Seed**: Set seed Ä‘á»ƒ reproducible results
3. **Performance**: Learned params slower nhÆ°ng chÃ­nh xÃ¡c hÆ¡n
4. **Validation**: LuÃ´n kiá»ƒm tra output trajectories match vá»›i real data

## ðŸ“š References

- Learning Curve: Ebbinghaus (1885), Anderson (2000)
- Q-Learning: Watkins & Dayan (1992)
- Student Modeling: Corbett & Anderson (1995)

## ðŸš€ Next Steps

1. âœ… Integrate vá»›i Q-learning training pipeline
2. âœ… Generate large-scale training data
3. âœ… Validate against real student logs
4. â³ Deploy for real-time recommendations
5. â³ A/B testing in production

---

**Version**: 2.0 (Enhanced)  
**Last Updated**: 2024-11-06  
**Author**: AI-Assisted Development
