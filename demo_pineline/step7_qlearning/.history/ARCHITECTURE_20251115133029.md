# ğŸ—ï¸ System Architecture

## ğŸ“ Kiáº¿n trÃºc tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ADAPTIVE LEARNING SYSTEM V2.0                     â”‚
â”‚           Q-Learning + LO Mastery Tracking                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ course_structure.json (37 modules)                            â”‚
â”‚  â€¢ cluster_profiles.json (6 clusters)                           â”‚
â”‚  â€¢ Po_Lo.json (LO â†’ Activity mappings)                         â”‚
â”‚  â€¢ midterm_lo_weights.json (LO importance)                      â”‚
â”‚  â€¢ log.csv, grade.csv (Moodle logs)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CORE LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ StateBuilderV2 (7D State)                                  â”‚ â”‚
â”‚  â”‚ (cluster, module, progress, score, phase, engagement, stuck)â”‚ â”‚
â”‚  â”‚ + convert_7d_to_6d() for Q-table compatibility            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ActionSpace (15 Actions)                                   â”‚ â”‚
â”‚  â”‚ (action_type, time_context: past/current/future)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ActivityRecommender                                         â”‚ â”‚
â”‚  â”‚ (Intelligent activity selection based on weak LOs)         â”‚ â”‚
â”‚  â”‚ + LO Improvement Prediction (EMA formula)                 â”‚ â”‚
â”‚  â”‚ + XAI Explanations                                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ RewardCalculatorV2 (12 Components)                         â”‚ â”‚
â”‚  â”‚ + LO Mastery Tracking & Midterm Prediction                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ QLearningAgentV2                                           â”‚ â”‚
â”‚  â”‚ (Q-table: 7,779+ states, Îµ-greedy policy)                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LOMasteryTracker                                           â”‚ â”‚
â”‚  â”‚ (Track LO mastery, predict midterm scores)                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SERVICE LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ ModelLoader (Load Q-table, components)                        â”‚
â”‚  â€¢ ClusterService (Predict cluster from features)                â”‚
â”‚  â€¢ RecommendationService (Generate top-K recommendations)         â”‚
â”‚  â€¢ QTableService (Q-table inspection)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API LAYER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ FastAPI (api_service.py)                                      â”‚
â”‚    - GET  /api/health                                            â”‚
â”‚    - GET  /api/model-info                                        â”‚
â”‚    - POST /api/recommend                                         â”‚
â”‚    - GET  /api/qtable/states/positive                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š State Representation (7D)

### State Vector
```python
state = (
    cluster_id,        # 0-4 (5 clusters, exclude teacher)
    module_idx,       # 0-5 (6 modules)
    progress_bin,     # 0.25, 0.5, 0.75, 1.0 (quartile)
    score_bin,        # 0.25, 0.5, 0.75, 1.0 (quartile)
    learning_phase,   # 0=pre, 1=active, 2=reflective
    engagement_level, # 0=low, 1=medium, 2=high
    frustration_level # 0 (currently disabled)
)
```

### State Space Size
```
Total possible states = 5 Ã— 6 Ã— 4 Ã— 4 Ã— 3 Ã— 3 Ã— 1 = 2,160 states
Trained states: 7,779+ (360% coverage - some states visited multiple times)
```

### State Building Process
1. **Cluster Mapping**: Map original cluster (0-6) â†’ training cluster (0-4)
2. **Module Mapping**: Map module_id (46-82) â†’ module_idx (0-5)
3. **Quartile Binning**: Continuous values â†’ discrete bins
4. **Phase Detection**: From recent action history
5. **Engagement Calculation**: From action quality weights

---

## ğŸ¯ Action Space (15 Actions)

### Action Structure
```python
LearningAction {
    action_type: str      # view_content, attempt_quiz, submit_quiz, etc.
    time_context: str     # past, current, future
    index: int           # 0-14 (action space index)
}
```

### Action Distribution
- **Past actions** (5): view_assignment, view_content, attempt_quiz, review_quiz, post_forum
- **Current actions** (7): view_assignment, view_content, submit_assignment, attempt_quiz, submit_quiz, review_quiz, post_forum
- **Future actions** (3): view_content, attempt_quiz, post_forum

### Activity Selection
Sau khi chá»n action type, `ActivityRecommender` sáº½:
1. Lá»c LO yáº¿u (mastery < threshold)
2. TÃ¬m activities phÃ¹ há»£p vá»›i action type vÃ  time context
3. Æ¯u tiÃªn activities target LO yáº¿u nhÆ°ng quan trá»ng cho midterm
4. Match difficulty vá»›i LO mastery level

---

## ğŸ’° Reward System (12 Components)

### Reward Components
1. **Completion Reward** (cluster-adaptive: 5.0/3.5/2.0)
2. **Score Improvement** (delta Ã— 5.0)
3. **High Score Bonus** (score â‰¥ 0.9)
4. **Progression Bonus** (difficulty-based)
5. **Time Efficiency** (strong clusters only)
6. **Learning Phase Bonus** (phase-appropriate actions)
7. **Engagement Bonus** (high engagement)
8. **Failure Penalty** (cluster-adaptive: -0.5/-1.0/-1.5)
9. **Stuck Penalty** (currently disabled)
10. **Low Engagement Penalty** (-0.5)
11. **Action Sequence Bonus** (beneficial sequences)
12. **LO Mastery Improvement** (Î”LO Ã— midterm_weight Ã— cluster_bonus)

### LO Mastery Reward Formula
```python
Î”LO_reward = Î£(
    Î”LO_mastery[lo] Ã— 
    midterm_weight[lo] Ã— 
    cluster_bonus Ã— 
    inverse_mastery_bonus Ã— 
    10.0
)
```

**Cluster Bonuses:**
- Weak: 1.5 (khuyáº¿n khÃ­ch nhiá»u hÆ¡n)
- Medium: 1.2
- Strong: 1.0

**Inverse Mastery Bonus:** `2.0 - old_mastery` (Æ°u tiÃªn cáº£i thiá»‡n LO yáº¿u)

---

## ğŸ§  Q-Learning Algorithm

### Update Rule
```python
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]

where:
- Î± (learning_rate) = 0.1
- Î³ (discount_factor) = 0.95
- r = reward from RewardCalculatorV2
- s' = next state
```

### Îµ-Greedy Policy
```python
if random() < epsilon (0.1):
    action = random_action()  # Exploration
else:
    action = argmax(Q[state, :])  # Exploitation
```

### Cluster-Adaptive Learning Rates
- Weak clusters: 0.15 (higher LR, need more adaptation)
- Medium clusters: 0.10
- Strong clusters: 0.08 (lower LR, more stable)

---

## ğŸ“ˆ LO Mastery Tracking

### Mastery Update
```python
# Exponential Moving Average
if success:
    new_mastery = old_mastery + Î± Ã— (outcome_score - old_mastery)
else:
    new_mastery = old_mastery - Î± Ã— 0.1

# Learning rate Î± depends on cluster
Î± = {weak: 0.3, medium: 0.2, strong: 0.15}
```

### Midterm Score Prediction
```python
predicted_score = Î£(LO_mastery[lo] Ã— midterm_weight[lo] Ã— total_marks)

potential_improvement = Î£(
    (target_mastery - current_mastery) Ã— 
    midterm_weight[lo] Ã— 
    total_marks
)
```

### Weak LO Identification
```python
weak_los = [
    (lo_id, mastery, weight)
    for lo_id, mastery in all_los
    if mastery < threshold (0.6)
]
# Sorted by priority = (1 - mastery) Ã— weight
```

---

## ğŸ”„ Data Flow

### Training Phase
```
Student â†’ State Builder â†’ Q-Agent (select action)
    â†“
Activity Recommender (filter weak LOs) â†’ Activity Selection
    â†“
Student.do_action() â†’ Outcome
    â†“
Reward Calculator (12 components + LO tracking)
    â†“
Q-Agent.update() â†’ Q-table
    â†“
LO Tracker.update_mastery() â†’ Mastery State
```

### Inference Phase (API)
```
Request â†’ State Builder â†’ Q-table Lookup
    â†“
Rank Actions by Q-value â†’ Top-K Recommendations
    â†“
Activity Recommender â†’ Specific Activities
    â†“
Response (with LO analysis, midterm prediction)
```

### Simulation Phase
```
LearningPathSimulator â†’ Student Simulation
    â†“
StateTransitionLogger â†’ Detailed Logging
    â†“
LOMasteryTracker â†’ LO Tracking & Prediction
    â†“
JSON Export â†’ Analysis & Database
```

---

## ğŸ—ƒï¸ Data Schema

### Course Structure
```json
{
  "modules": [
    {
      "id": 67,
      "name": "Quiz tuáº§n 2",
      "type": "do_quiz",
      "section": "Tuáº§n 2",
      "difficulty": "medium"
    }
  ]
}
```

### LO Mappings (Po_Lo.json)
```json
{
  "learning_outcomes": [
    {
      "id": "LO1.1",
      "description": "Nháº­n biáº¿t vÃ  giáº£i thÃ­ch cÃ¡c khÃ¡i niá»‡m cÆ¡ báº£n vá» AI",
      "activity_ids": [54, 98]
    }
  ]
}
```

### Midterm Weights
```json
{
  "lo_weights": {
    "LO1.1": 0.10,
    "LO1.2": 0.15,
    "LO2.2": 0.25,
    "LO2.4": 0.25
  }
}
```

---

## ğŸ” Production Considerations

### Performance
- Singleton pattern for model loader
- State caching for repeated queries
- Batch processing support
- Lazy loading of resources

### Scalability
- Stateless API design
- Supports concurrent requests
- Can deploy multiple instances
- JSON output ready for database migration

### Error Handling
- Graceful fallback for missing states
- Default recommendations for edge cases
- Comprehensive logging
- Health checks

---

## ğŸ“ˆ Metrics

### Training Metrics
- Q-table size: 7,779+ states
- Coverage: 18.2%+
- Average reward: 195.66+
- Exploration rate: ~10%

### API Metrics
- Response time: < 50ms
- Success rate: > 95%

### LO Tracking Metrics
- LO mastery updates per activity
- Midterm score predictions
- Weak LO identification accuracy

---

For usage details, see **USAGE_GUIDE.md**  
For quick start, see **README.md**
