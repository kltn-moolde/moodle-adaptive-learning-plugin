\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% Template version as of 6/27/2024

\usepackage{cite}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{amsmath,amsxtra,amssymb,amsthm,latexsym,amscd,amsfonts}
\usepackage[utf8]{vietnam}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{multirow}
\usepackage{booktabs} % Để kẻ bảng đẹp hơn
\usepackage{url}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\sectionmark}[1]{\markright{\MakeUppercase{#1}}{}}

% Điều chỉnh lề theo template FJCAI
\setlength{\oddsidemargin}{0.5pt}
\addtolength{\textwidth}{-0.5cm}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\makeatletter
\def\ps@IEEEtitlepagestyle{%
\def\@oddhead{\hfil \small{\textit{Hội thảo khoa học Quốc gia về Trí tuệ nhân tạo (FJCAI) - Cần Thơ, 27-28/3/2026}\hfil}%
	\def\@evenhead{\hfil\small{\textit{Hội thảo khoa học quốc gia về Trí tuệ nhân tạo (FJCAI) - Cần Thơ, 27-28/3/2026}\hfil}}%
		\def\@oddfoot{\scriptsize \thepage \hfil }%
		\def\@evenfoot{\scriptsize \hfil \thepage}
	}
}
\makeatother

\fancyhf{}
\fancyhead[RE,LO]{\centering{\small{\textit{Hội thảo khoa học Quốc gia về Trí tuệ nhân tạo (FJCAI) - Cần Thơ, 27-28/3/2026}}}}

\begin{document}

\title{Ứng dụng Trí tuệ nhân tạo trong xây dựng hệ thống Học tăng cường hỗ trợ dạy và học STEM}

\author{\IEEEauthorblockN{1\textsuperscript{st} abc}
\IEEEauthorblockA{\textit{Khoa Công nghệ Thông tin} \\
\textit{Trường Đại học Sài Gòn}\\
TP. Hồ Chí Minh, Việt Nam \\
emaik@sgu.edu.vn}
\and
\IEEEauthorblockN{2\textsuperscript{nd} ABC}
\IEEEauthorblockA{\textit{Khoa Công nghệ Thông tin} \\
\textit{Trường Đại học Sài Gòn}\\
TP. Hồ Chí Minh, Việt Nam \\
email@sgu.edu.vn}
\and
\IEEEauthorblockN{3\textsuperscript{rd} ABC}
\IEEEauthorblockA{\textit{Khoa Công nghệ Thông tin} \\
\textit{Trường Đại học Sài Gòn}\\
TP. Hồ Chí Minh, Việt Nam \\
email@sgu.edu.vn}
}

\maketitle

\begin{abstract}
Trong kỷ nguyên công nghiệp 4.0, giáo dục STEM đóng vai trò then chốt trong việc đào tạo nguồn nhân lực chất lượng cao. Tuy nhiên, các phương pháp giảng dạy truyền thống và hệ thống quản lý học tập (LMS) hiện hành thường áp dụng cách tiếp cận ``một kích cỡ cho tất cả'', thất bại trong việc đáp ứng nhu cầu cá nhân hóa của từng người học. Bài báo này đề xuất một hệ thống gợi ý lộ trình học tập thông minh sử dụng kỹ thuật Học tăng cường (Reinforcement Learning - RL), cụ thể là thuật toán Q-learning, được tích hợp vào nền tảng Moodle qua chuẩn LTI 1.3. Hệ thống mô hình hóa quá trình học tập dưới dạng Quy trình quyết định Markov (MDP), sử dụng dữ liệu hành vi thực tế để phân cụm người học và tối ưu hóa chiến lược gợi ý. Kết quả thực nghiệm mô phỏng trên 500 vòng lặp cho thấy thuật toán giúp tăng 22.5\% điểm số trung bình và giảm 51.0\% số lượng kỹ năng yếu so với phương pháp truyền thống.
\end{abstract}

\begin{IEEEkeywords}
Học tăng cường, Q-learning, Cá nhân hóa học tập, Giáo dục STEM, Microservices, LTI 1.3.
\end{IEEEkeywords}

\section{Giới thiệu}
\label{sec:intro}
Sự phát triển mạnh mẽ của Trí tuệ nhân tạo (AI) đang định hình lại nhiều lĩnh vực, trong đó có giáo dục. [cite\_start]Theo nghiên cứu của Frey và Osborne, khoảng 47\% các công việc truyền thống có nguy cơ bị tự động hóa, đặt ra yêu cầu cấp thiết về việc trang bị các kỹ năng mới cho người lao động, đặc biệt là các kỹ năng STEM (Khoa học, Công nghệ, Kỹ thuật và Toán học)\cite{b2}. Giáo dục STEM chú trọng phát triển tư duy phản biện và khả năng giải quyết vấn đề, tuy nhiên, việc triển khai hiệu quả gặp nhiều rào cản do sự đa dạng về năng lực và tốc độ tiếp thu của học viên.

Thách thức lớn nhất hiện nay là cá nhân hóa trải nghiệm học tập (Personalized Adaptive Learning - PAL) trên quy mô lớn. [cite\_start]Các hệ thống LMS truyền thống như Moodle, Blackboard chủ yếu đóng vai trò lưu trữ tài liệu và quản lý điểm số, thiếu khả năng phân tích hành vi để đưa ra các can thiệp sư phạm kịp thời\cite{b1}. [cite\_start]Tại Việt Nam, các nghiên cứu về ứng dụng AI trong giáo dục chủ yếu tập trung vào bài toán dự báo (prediction) - ví dụ như dự báo nguy cơ bỏ học hoặc dự đoán điểm số cuối kỳ - mà chưa chú trọng nhiều đến bài toán đưa ra khuyến nghị hành động (prescription) để cải thiện kết quả đó\cite{b4}.

Để giải quyết vấn đề này, nhu cầu về một hệ thống hỗ trợ dạy và học STEM cá nhân hóa ứng dụng Học tăng cường (Reinforcement Learning - RL) trở nên cấp thiết. RL, với cơ chế học thử-sai (trial-and-error), cho phép hệ thống tự động khám phá và tối ưu hóa chiến lược giảng dạy dựa trên phản hồi liên tục từ người học.

Nghiên cứu này đề xuất xây dựng một hệ thống gợi ý thông minh tích hợp vào Moodle LMS, với các đóng góp chính sau:
\begin{enumerate}
    \item Đề xuất kiến trúc Microservices tích hợp qua chuẩn LTI 1.3, đảm bảo khả năng mở rộng và tương thích với nhiều nền tảng LMS.
    \item Xây dựng quy trình xử lý dữ liệu và phân cụm người học để giải quyết bài toán không gian trạng thái trong RL.
    \item Thiết kế và tối ưu hóa thuật toán Q-learning với hàm phần thưởng đa mục tiêu, thích ứng với đặc điểm của từng nhóm người học.
    \item Kiểm chứng hiệu quả của hệ thống thông qua mô phỏng so sánh (A/B testing) với dữ liệu tham số hóa từ thực tế.
\end{enumerate}

\section{Tổng quan nghiên cứu}
\label{sec:related_work}

\subsection{Hệ thống học tập thích ứng (Adaptive Learning)}
Các hệ thống PAL điều chỉnh lộ trình học tập dựa trên nhu cầu riêng biệt của sinh viên. [cite\_start]Theo báo cáo tổng quan của du Plooy và cộng sự (2024), khảo sát 69 công trình nghiên cứu, 59\% số nghiên cứu ghi nhận sự cải thiện về kết quả học tập và 36\% chỉ ra sự gia tăng mức độ tham gia khi áp dụng PAL\cite{b1}. Các nền tảng như Moodle và McGraw-Hill’s Connect LearnSmart là những môi trường phổ biến để triển khai các giải pháp này.

\subsection{Học tăng cường trong giáo dục}
Trong những năm gần đây, Học tăng cường (RL) đã nổi lên như một phương pháp hiệu quả để xây dựng các gia sư thông minh (Intelligent Tutoring Systems). [cite\_start]Theo Riedmann (2025), xu hướng sử dụng RL trong giáo dục tăng mạnh từ năm 2018, đặc biệt trong lĩnh vực STEM\cite{b3}. [cite\_start]Về mặt thuật toán, Q-learning và Deep Q-Network (DQN) là phổ biến nhất nhờ tính linh hoạt của phương pháp Model-free, cho phép hệ thống học chiến lược tối ưu mà không cần mô hình hóa chính xác quy luật phức tạp của hành vi con người\cite{b6}.

[cite\_start]Tuy nhiên, Riedmann cũng chỉ ra hạn chế lớn của các nghiên cứu hiện tại là sự phụ thuộc vào dữ liệu mô phỏng và thiếu các triển khai thực tế tại các quốc gia đang phát triển\cite{b3}.

\subsection{Tình hình nghiên cứu tại Việt Nam}
Tại Việt Nam, các nghiên cứu chủ yếu tập trung vào khai phá dữ liệu (Data Mining) để dự báo. [cite\_start]Ví dụ, nghiên cứu của Trần Bá Thuấn (2021) sử dụng Random Forest để dự báo cảnh báo học vụ, hay Lưu Hoài Sang (2020) sử dụng Deep Learning để dự báo điểm số\cite{b4}. Mặc dù đạt độ chính xác cao, các mô hình này mang tính chất ``dự báo tĩnh'' và thiếu cơ chế gợi ý động theo thời gian thực. Đề tài này nhằm lấp đầy khoảng trống đó bằng cách xây dựng một hệ thống RL có khả năng tương tác và điều hướng người học.

\section{Kiến trúc hệ thống và Xử lý dữ liệu}
\label{sec:system}

\subsection{Kiến trúc Microservices}
Hệ thống được thiết kế theo kiến trúc Microservices để đảm bảo tính module hóa và khả năng mở rộng độc lập. Các thành phần chính bao gồm:
\begin{itemize}
    \item \textbf{LTI Integration Service:} Đảm nhiệm việc xác thực và kết nối an toàn với Moodle LMS thông qua chuẩn LTI 1.3 và giao thức OAuth 2.0. [cite\_start]Service này giúp hệ thống hoạt động như một công cụ độc lập (Tool Provider) mà không cần can thiệp vào mã nguồn của LMS\cite{b7}.
    \item \textbf{Recommend Service:} Là trái tim của hệ thống, chứa thuật toán Q-learning (được viết bằng Python). [cite\_start]Service này chịu trách nhiệm tính toán giá trị Q, cập nhật bảng tri thức và trả về danh sách gợi ý\cite{b5}.
    \item \textbf{Course Service \& User Service:} Quản lý metadata cấu trúc khóa học và thông tin người dùng, đồng bộ hóa từ LMS.
    \item \textbf{Frontend:} Ứng dụng phía người dùng xây dựng bằng ReactJS, hiển thị dashboard cá nhân hóa.
\end{itemize}
[cite\_start]Dữ liệu được lưu trữ phân tán, với MongoDB được sử dụng cho Recommend Service để xử lý dữ liệu log hành vi phi cấu trúc và trạng thái người học thay đổi liên tục\cite{b5}.

\subsection{Quy trình Xử lý dữ liệu và Phân cụm}
[cite\_start]Để xây dựng không gian trạng thái cho thuật toán RL, chúng tôi thực hiện quy trình khai phá dữ liệu từ log hệ thống Moodle của một khóa học STEM thực tế (Course ID 670) với 13,995 sự kiện tương tác và 23 sinh viên tham gia đầy đủ\cite{b5}.

\subsubsection{Tiền xử lý dữ liệu}
Dữ liệu thô từ Moodle chứa nhiều nhiễu. Quá trình làm sạch bao gồm việc loại bỏ các sự kiện hệ thống tự động (như \textit{webservice\_function\_called}) và chuẩn hóa thời gian. [cite\_start]Sau đó, chúng tôi trích xuất 114 đặc trưng hành vi và áp dụng kỹ thuật lọc tương quan (Correlation Filtering) để loại bỏ hiện tượng đa cộng tuyến, giữ lại 15 đặc trưng quan trọng nhất như: tần suất xem tài liệu, số lần nộp bài, thời gian phản hồi, v.v. \cite{b5}.

\subsubsection{Phân cụm người học (Clustering)}
Việc cá nhân hóa đòi hỏi hệ thống phải nhận diện được đặc điểm của từng nhóm đối tượng. Chúng tôi sử dụng thuật toán K-means để phân nhóm sinh viên dựa trên 15 đặc trưng đã trích xuất.
[cite\_start]Số lượng cụm tối ưu $K=6$ được xác định thông qua phương pháp Elbow và chỉ số Silhouette\cite{b5}. Các nhóm người học được định danh bao gồm:
\begin{itemize}
    \item \textbf{Cluster 0 (Nhóm cần hỗ trợ - 34.8\%):} Tương tác thấp, thụ động, điểm số thấp.
    \item \textbf{Cluster 1 (Nhóm tự giác):} Thường xuyên theo dõi tiến độ và bảng điểm.
    \item \textbf{Cluster 2 (Nhóm chủ động - 30.4\%):} Tương tác cao, nộp bài đầy đủ.
    \item \textbf{Cluster 4 (Nhóm nghiên cứu):} Tập trung vào việc xem và tải tài liệu.
    [cite\_start]\item \textbf{Cluster 5 (Nhóm thành tích):} Quan tâm đến huy hiệu và xếp hạng \cite{b5}.
\end{itemize}
Thông tin Cluster ID này sẽ được đưa vào vector trạng thái của thuật toán Q-learning.

\section{Mô hình hóa bài toán Q-learning}
\label{sec:methodology}

Bài toán gợi ý lộ trình học tập được mô hình hóa dưới dạng Quy trình Quyết định Markov (MDP), bao gồm bộ ba $<S, A, R>$.

\subsection{Không gian trạng thái (State Space)}
Trạng thái $S_t$ tại thời điểm $t$ đại diện cho tình trạng học tập hiện tại của sinh viên, được định nghĩa là một vector 6 chiều:
\begin{equation}
S_t = (C, M, P, Sc, Ph, E)
\end{equation}
Trong đó:
\begin{itemize}
    \item $C$ (Cluster): Nhóm người học ($0 \dots 5$) từ kết quả phân cụm.
    \item $M$ (Module): Chỉ số bài học hiện tại trong lộ trình tuần tự.
    \item $P$ (Progress): Mức độ hoàn thành module, được rời rạc hóa thành 4 mức (0.25, 0.5, 0.75, 1.0).
    \item $Sc$ (Score): Điểm số tích lũy, chia thành 4 mức tương ứng (Yếu, TB, Khá, Giỏi).
    \item $Ph$ (Phase): Giai đoạn học tập (0: Pre-learning, 1: Active-learning, 2: Reflective-learning).
    [cite\_start]\item $E$ (Engagement): Mức độ tương tác (Thấp, TB, Cao), tính toán dựa trên trọng số hành động theo khung ICAP \cite{b5}.
\end{itemize}
Kích thước không gian trạng thái là $5 \times 6 \times 4 \times 4 \times 3 \times 3 \approx 4,320$ trạng thái, đảm bảo tính khả thi cho việc hội tụ của bảng Q-table.

\subsection{Không gian hành động (Action Space)}
Thay vì sử dụng hàng trăm sự kiện thô của Moodle, chúng tôi thiết kế 15 hành động sư phạm cốt lõi, được phân loại theo ngữ cảnh thời gian để hỗ trợ các chiến lược học tập khác nhau (Bảng \ref{tab:actions}).

\begin{table}[htbp]
\caption{Không gian hành động theo ngữ cảnh thời gian}
\begin{center}
\begin{tabular}{|c|l|p{4cm}|}
\hline
\textbf{Ngữ cảnh} & \textbf{Hành động} & \textbf{Mục tiêu sư phạm} \\
\hline
\textbf{PAST} & view\_content\_past & Ôn tập kiến thức cũ (Remediation) \\
(Quá khứ) & review\_quiz\_past & Phân tích lỗi sai \\
& attempt\_quiz\_past & Cải thiện kỹ năng còn yếu \\
& post\_forum\_past & Thảo luận lại chủ đề cũ \\
\hline
\textbf{CURRENT} & view\_assignment & Hiểu yêu cầu nhiệm vụ \\
(Hiện tại) & view\_content & Tiếp thu kiến thức mới \\
& attempt\_quiz & Thực hành kiến tạo (Constructive) \\
& submit\_quiz & Hoàn thành đánh giá \\
& post\_forum & Tương tác xã hội \\
\hline
\textbf{FUTURE} & view\_content\_fut & Chuẩn bị bài trước (Preview) \\
(Tương lai) & attempt\_quiz\_fut & Thử thách nâng cao (Exploration) \\
\hline
\end{tabular}
\label{tab:actions}
\end{center}
\end{table}

[cite\_start]Việc phân chia này cho phép AI đưa ra các chiến lược linh hoạt: Gợi ý hành động Past cho sinh viên yếu cần củng cố nền tảng, và hành động Future cho sinh viên giỏi muốn học vượt \cite{b5}.

\subsection{Hàm phần thưởng (Reward Function)}
Hàm thưởng là thành phần quan trọng nhất định hướng hành vi của tác nhân. Chúng tôi thiết kế hàm thưởng tổng hợp $R_{total}$:
\begin{equation}
R_{total} = R_{base} + R_{LO} + R_{seq} - P_{penalty}
\end{equation}

\subsubsection{Phần thưởng cơ bản thích ứng ($R_{base}$)}
Áp dụng chiến lược Cluster-Adaptive Reward. Nhóm sinh viên Yếu (Weak Cluster) nhận được điểm thưởng cao hơn (+10) khi hoàn thành một hành động so với nhóm Giỏi (+5). [cite\_start]Cơ chế này nhằm tạo động lực ngoại sinh, khuyến khích nhóm yếu duy trì tương tác thay vì bỏ cuộc\cite{b6}.

\subsubsection{Phần thưởng dựa trên Chuẩn đầu ra ($R_{LO}$)}
Được tính dựa trên mức độ cải thiện năng lực ($\Delta Mastery$) của sinh viên đối với các Chuẩn đầu ra (LO).
\begin{equation}
R_{LO} = \sum (\Delta Mastery_i \times W_{midterm, i} \times K_{cluster})
\end{equation}
[cite\_start]Trong đó, $W_{midterm}$ là trọng số quan trọng của LO trong bài thi, và $K_{cluster}$ là hệ số khích lệ riêng cho từng nhóm\cite{b5}.

\subsubsection{Phần thưởng chuỗi hành động ($R_{seq}$)}
Hệ thống thưởng thêm điểm cho các chuỗi hành động hợp lý về mặt sư phạm (Beneficial Sequences), ví dụ: \textit{view\_content} $\rightarrow$ \textit{attempt\_quiz} (Học đi đôi với hành), hoặc \textit{review\_quiz} $\rightarrow$ \textit{view\_content} (Phản tư và học lại).

\subsection{Thuật toán Q-learning}
Hệ thống sử dụng thuật toán Q-learning tiêu chuẩn để cập nhật bảng giá trị Q:
\begin{equation}
Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]
\end{equation}
Với tốc độ học $\alpha=0.1$ và hệ số chiết khấu $\gamma=0.95$. [cite\_start]Chiến lược lựa chọn hành động là $\epsilon$-greedy, với $\epsilon$ giảm dần từ 1.0 xuống 0.01 sau 400 episodes để cân bằng giữa khám phá và khai thác \cite{b6}.

\section{Thực nghiệm và Kết quả}
\label{sec:experiments}

Do giới hạn về việc triển khai trên sinh viên thực tế trong thời gian ngắn, nghiên cứu sử dụng phương pháp mô phỏng dựa trên dữ liệu (Data-driven Simulation) để giải quyết vấn đề ``khởi động lạnh'' và kiểm chứng thuật toán.

\subsection{Thiết lập môi trường mô phỏng}
[cite\_start]Môi trường giả lập được xây dựng dựa trên các tham số thống kê (xác suất chuyển trạng thái, phân phối điểm số) trích xuất từ dữ liệu khóa học 670. Các tác nhân ảo (Virtual Agents) được sinh ra với phân phối năng lực mô phỏng thực tế: 20\% Yếu, 60\% Trung bình, 20\% Giỏi \cite{b5}.
Quá trình huấn luyện diễn ra qua 500 vòng lặp (episodes), mỗi vòng gồm 100 tác nhân.

\subsection{Đánh giá so sánh (A/B Testing)}
Chúng tôi so sánh hiệu quả giữa hai chính sách:
\begin{itemize}
    \item \textbf{Nhóm Q-learning (Thực nghiệm):} Hành động dựa trên bảng Q-table đã huấn luyện.
    \item \textbf{Nhóm Historical Policy (Đối chứng):} Hành động dựa trên xác suất ngẫu nhiên mô phỏng lại thói quen của sinh viên các khóa trước.
\end{itemize}

Kết quả định lượng (Bảng \ref{tab:results}) cho thấy sự vượt trội rõ rệt của thuật toán Q-learning trên tất cả các chỉ số đo lường.

\begin{table}[htbp]
\caption{Kết quả so sánh hiệu suất giữa hai nhóm}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Chỉ số (Metrics)} & \textbf{Đối chứng} & \textbf{Thực nghiệm} & \textbf{Cải thiện (\%)} \\
\midrule
Tổng phần thưởng tích lũy & 88.4 & 389.6 & \textbf{+340.8\%} \\
Điểm số trung bình (thang 10) & 6.25 & 7.66 & \textbf{+22.5\%} \\
Mức độ thành thạo LO (0-1) & 0.58 & 0.66 & \textbf{+13.9\%} \\
Số kỹ năng yếu (Weak LOs) & 3.02 & 1.48 & \textbf{-51.0\%} \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{center}
\end{table}

\subsection{Phân tích tác động sư phạm}
Bên cạnh các con số thống kê, hệ thống thể hiện khả năng thích ứng thông minh với từng nhóm đối tượng, giải quyết được vấn đề ``một kích cỡ cho tất cả'':

\subsubsection{Đối với nhóm sinh viên Yếu}
Chỉ số quan trọng nhất là \textbf{Số kỹ năng yếu (Avg Weak LO Count)} đã giảm mạnh 51.0\% (từ 3.02 xuống 1.48). [cite\_start]Điều này chứng minh rằng AI đã nhận diện được các lỗ hổng kiến thức và chủ động gợi ý các hành động khắc phục (Remedial Actions) như xem lại bài giảng cũ, thay vì đẩy sinh viên vào làm các bài kiểm tra quá sức gây nản lòng\cite{b5}. Mức chênh lệch phần thưởng lớn nhất cũng được ghi nhận ở nhóm này, phản ánh chiến lược ``khích lệ'' của hàm thưởng.

\subsubsection{Đối với nhóm sinh viên Giỏi}
Nhóm này đạt điểm số trung bình tuyệt đối cao nhất (8.18/10). [cite\_start]Hệ thống đã nhận diện năng lực vượt trội và chuyển sang chiến lược đề xuất các nội dung thách thức hơn (Future Actions) hoặc các bài tập khó, giúp duy trì hứng thú học tập và tránh sự nhàm chán\cite{b5}.

\subsubsection{Độ tin cậy thống kê}
[cite\_start]Kiểm định T-test độc lập giữa hai nhóm cho kết quả $T-statistic = 67.74$ và $P-value \approx 0.000$, khẳng định sự khác biệt về hiệu quả là có ý nghĩa thống kê với độ tin cậy 99.9\%\cite{b5}.

\section{Kết luận và Hướng phát triển}
\label{sec:conclusion}

Bài báo đã trình bày một giải pháp toàn diện để cá nhân hóa giáo dục STEM thông qua việc ứng dụng Học tăng cường. Các đóng góp chính bao gồm: (1) Kiến trúc hệ thống mở dựa trên Microservices và LTI 1.3; (2) Quy trình mô hình hóa dữ liệu người học chi tiết; và (3) Thuật toán Q-learning với cơ chế thưởng thích ứng.

Kết quả thực nghiệm cho thấy hệ thống không chỉ cải thiện điểm số (+22.5\%) mà quan trọng hơn là giúp lấp đầy các lỗ hổng kiến thức cho sinh viên yếu (-51\% số kỹ năng yếu), hiện thực hóa mục tiêu ``không ai bị bỏ lại phía sau''.

Tuy nhiên, nghiên cứu vẫn còn hạn chế khi chưa được triển khai trên lớp học thực tế (Live Deployment) và không gian trạng thái bị giới hạn bởi phương pháp rời rạc hóa.

Hướng phát triển trong tương lai bao gồm:
\begin{itemize}
    [cite\_start]\item \textbf{Deep Reinforcement Learning (DRL):} Áp dụng mạng nơ-ron sâu (DQN, PPO) để xử lý không gian trạng thái liên tục và phức tạp hơn\cite{b6}.
    \item \textbf{Triển khai thực tế:} Tích hợp hệ thống vào các khóa học STEM tại trường đại học để thu thập dữ liệu phản hồi thực và tinh chỉnh mô hình.
    [cite\_start]\item \textbf{Federated Learning:} Nghiên cứu cơ chế học tập liên kết để bảo vệ quyền riêng tư dữ liệu người học khi triển khai trên nhiều cơ sở giáo dục\cite{b5}.
\end{itemize}

\section*{Lời cảm ơn}
Nhóm tác giả xin chân thành cảm ơn TS. Đỗ Như Tài đã hướng dẫn tận tình. Nghiên cứu được thực hiện tại Khoa Công nghệ Thông tin, Trường Đại học Sài Gòn.

\begin{thebibliography}{00}
\bibitem{b1} E. du Plooy et al., ``Personalized adaptive learning in higher education: A scoping review,'' \textit{Heliyon}, vol. 10, no. 21, p. e39630, 2024.
\bibitem{b2} C. B. Frey and M. A. Osborne, ``The future of employment: How susceptible are jobs to computerisation?,'' \textit{Tech. Forecast. Soc. Change}, vol. 114, pp. 254--280, 2017.
\bibitem{b3} A. Riedmann et al., ``Reinforcement Learning in Education: A Systematic Literature Review,'' \textit{Int. J. Artif. Intell. Educ.}, 2025.
\bibitem{b4} T. B. Thuan, ``Ứng dụng machine learning dự báo sinh viên diện cảnh báo học tập,'' \textit{Hue Uni. Journal of Science}, 2021.
\bibitem{b5} I. Gligorea et al., ``Adaptive Learning Using Artificial Intelligence in e-Learning,'' \textit{Educ. Sci.}, vol. 13, no. 12, 2023.
\bibitem{b6} R. S. Sutton and A. G. Barto, \textit{Reinforcement Learning: An Introduction}. MIT Press, 2018.
\bibitem{b7} IMS Global, ``LTI 1.3 Implementation Guide,'' [Online]. Available: https://www.imsglobal.org/spec/lti/v1p3.
\end{thebibliography}

\end{document}