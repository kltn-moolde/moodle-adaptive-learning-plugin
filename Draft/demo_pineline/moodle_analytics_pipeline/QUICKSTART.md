# Quick Start Guide - KMeans-Only Pipeline

## ğŸš€ Get Started in 5 Minutes

### Step 1: Installation

```bash
cd demo_pineline/moodle_analytics_pipeline
pip install -r requirements.txt
```

### Step 2: Run Pipeline

```bash
python main.py
```

Done! Pipeline sáº½ tá»± Ä‘á»™ng:
1. Extract features tá»« data
2. Select optimal features
3. Find optimal number of clusters (KMeans + Voting)
4. Profile clusters with AI
5. Visualize results

---

## ğŸ“Š Check Results

After running, check `outputs/` directory:

```bash
outputs/
â”œâ”€â”€ features/                    # âœ… Extracted features
â”œâ”€â”€ feature_selection/           # âœ… Selected features + analysis
â”œâ”€â”€ optimal_clusters/            # âœ… Optimal k + evaluation plots
â”œâ”€â”€ cluster_profiling/           # âœ… AI cluster profiles
â””â”€â”€ comparison/                  # âœ… Visualizations
```

### Key Files to Check:

1. **Feature Selection Report**:
   ```bash
   cat outputs/feature_selection/feature_selection_report.txt
   ```

2. **Optimal Clusters Report**:
   ```bash
   cat outputs/optimal_clusters/optimal_clusters_report.txt
   ```

3. **Cluster Profiles**:
   ```bash
   cat outputs/cluster_profiling/cluster_profiles_report.txt
   ```

---

## ğŸ¯ Expected Output

### Console Output Example:

```
================================================================================
MOODLE ANALYTICS PIPELINE - KMEANS-ONLY EXECUTION
================================================================================

ğŸ“Š PHASE 1: Feature Extraction
--------------------------------------------------------------------------------
âœ“ Loaded 150 real students with 25 features

ğŸ” PHASE 2: Feature Selection (Variance + Correlation Filtering)
--------------------------------------------------------------------------------
  âœ“ Retained: 18 features (variance filter)
  âœ— Removed: 7 low-variance features
  âœ“ Retained: 15 features (correlation filter)
  âœ— Removed: 3 highly-correlated features
âœ… SELECTED 15 OPTIMAL FEATURES

ğŸ¯ PHASE 3: Finding Optimal Number of Clusters (KMeans + Voting)
--------------------------------------------------------------------------------
Testing k from 2 to 10...
Evaluating k=2... Elbow: 1245.67, Silhouette: 0.523, DB: 0.87
Evaluating k=3... Elbow: 1156.34, Silhouette: 0.612, DB: 0.65
Evaluating k=4... Elbow: 1189.45, Silhouette: 0.587, DB: 0.72
...
ğŸ¯ OPTIMAL K: 3
   Silhouette: 0.612
   Davies-Bouldin: 0.65

ğŸ¤– PHASE 4: Cluster Profiling with AI
--------------------------------------------------------------------------------
âœ“ Generated AI-powered profiles for 3 clusters
  Cluster 0: High performers (32.5%)
  Cluster 1: Medium performers (37.5%)
  Cluster 2: Struggling learners (30.0%)

ğŸ“ˆ PHASE 5: Visualization
--------------------------------------------------------------------------------
Generating visualizations...
  âœ“ Feature distributions
  âœ“ Cluster separation plots
  âœ“ Comparison summary

================================================================================
âœ… PIPELINE COMPLETED SUCCESSFULLY!
================================================================================

ğŸ“Š PIPELINE SUMMARY (KMeans-Only)
================================================================================
Real students:        150
Optimal clusters (k): 3
Features extracted:   25
Features selected:    15

âœ… All outputs saved to 'outputs/' directory
================================================================================
```

---

## ğŸ¨ Visualizations Generated

### 1. Feature Selection Analysis
![Feature Selection](outputs/feature_selection/feature_selection_analysis.png)
- Top features by importance
- Variance distribution
- Correlation heatmap
- Selection summary

### 2. Optimal Clusters Evaluation
![Optimal Clusters](outputs/optimal_clusters/optimal_clusters_evaluation.png)
- BIC curve (lower is better)
- AIC curve (lower is better)
- Silhouette score (higher is better)
- Composite score

### 3. Cluster Visualizations
![Cluster Viz](outputs/comparison/cluster_visualization.png)
- PCA visualization of clusters
- Feature distributions per cluster
- Cluster separation metrics

---

## âš™ï¸ Customization Examples

### Example 1: Enable/Disable LLM Profiling

```python
from core import MoodleAnalyticsPipeline

pipeline = MoodleAnalyticsPipeline()
results = pipeline.run_full_pipeline(
    grades_path='../data/grades.csv',
    logs_path='../data/logs.csv',
    enable_llm_profiling=True,  # â† Toggle AI profiling
    llm_provider='gemini'       # â† 'gemini' or 'openai'
)
```

### Example 2: Adjust Feature Selection Thresholds

```python
results = pipeline.run_full_pipeline(
    grades_path='../data/grades.csv',
    logs_path='../data/logs.csv',
    variance_threshold=0.02,      # â† Stricter (more filtering)
    correlation_threshold=0.90,   # â† Stricter (remove more redundant)
    max_features=10              # â† Limit to top 10
)
```

### Example 3: Test Different K Range

```python
results = pipeline.run_full_pipeline(
    grades_path='../data/grades.csv',
    logs_path='../data/logs.csv',
    k_range=range(3, 8)  # â† Only test k=3 to k=7
)
```

### Example 4: Use Only Selected Modules

```python
from core import FeatureSelector, OptimalClusterFinder
import pandas as pd
import json

# Load features
with open('outputs/features/features_scaled.json', 'r') as f:
    features = pd.DataFrame(json.load(f))

# Select features
selector = FeatureSelector(variance_threshold=0.01)
selected = selector.process_pipeline(features, 'outputs/feature_selection')

# Find optimal clusters
finder = OptimalClusterFinder(k_range=range(2, 8))
optimal_k, gmm = finder.process_pipeline(
    X=features[selected].values,
    output_dir='outputs/optimal_clusters'
)

print(f"Optimal K: {optimal_k}")
```

---

## ğŸ” Understanding Results

### Quality Score Interpretation

| Score Range | Grade      | Interpretation |
|-------------|------------|----------------|
| 85-100%     | Excellent  | Synthetic data ráº¥t giá»‘ng real data |
| 70-84%      | Good       | Synthetic data tÆ°Æ¡ng Ä‘á»‘i giá»‘ng vá»›i minor differences |
| 50-69%      | Fair       | Synthetic data cÃ³ moderate similarity |
| <50%        | Poor       | Synthetic data khÃ¡c biá»‡t Ä‘Ã¡ng ká»ƒ |

### KS Test Results

- **p-value > 0.05**: âœ“ PASS - Distributions are similar
- **p-value â‰¤ 0.05**: âœ— FAIL - Distributions differ significantly

**Pass Rate**: Sá»‘ features pass / Total features

### Optimal K Selection

Pipeline tá»± Ä‘á»™ng chá»n k dá»±a trÃªn:
- **50% BIC score** (normalized, inverted)
- **50% Silhouette score** (normalized)

K vá»›i composite score cao nháº¥t sáº½ Ä‘Æ°á»£c chá»n.

---

## ğŸ› Troubleshooting

### Issue 1: "No features passed variance threshold"

**Solution**: Lower variance_threshold
```python
variance_threshold=0.001  # Instead of 0.01
```

### Issue 2: "Quality score too low (<50%)"

**Possible causes**:
- Data cÃ³ nhiá»u noise
- Features khÃ´ng representative
- K khÃ´ng optimal

**Solutions**:
1. Increase k_range: `k_range=range(2, 15)`
2. Adjust feature selection thresholds
3. Check input data quality

### Issue 3: "ImportError: No module named 'sklearn'"

**Solution**: Install dependencies
```bash
pip install -r requirements.txt
```

### Issue 4: Pipeline runs but no outputs

**Check**:
```python
import os
print(os.path.exists('outputs'))  # Should be True
```

**Fix**:
```python
pipeline = MoodleAnalyticsPipeline(base_output_dir='./outputs')
```

---

## ğŸ“ Next Steps

1. **Review Validation Report**:
   - Check KS test pass rate
   - Review distribution comparisons
   - Examine correlation similarity

2. **Analyze Visualizations**:
   - Feature selection analysis
   - Optimal clusters evaluation
   - Real vs synthetic comparison

3. **Adjust Parameters**:
   - Fine-tune thresholds based on results
   - Test different k ranges
   - Experiment with feature selection

4. **Use Synthetic Data**:
   - Load from `outputs/gmm_generation/synthetic_students_gmm.csv`
   - Use for testing, simulation, or augmentation

---

## ğŸ“š Additional Resources

- **README_GMM.md**: Full documentation
- **MODULE_SUMMARY.md**: Detailed module docs
- **METRICS_GUIDE.md**: Metrics explanation
- **config.py**: All configurable parameters

---

## ğŸ¤ Need Help?

- Check logs in `pipeline.log`
- Review error messages in console
- Inspect intermediate outputs in `outputs/` subdirectories

---

**Happy Analyzing! ğŸ‰**
