### H∆∞·ªõng D·∫´n Step-by-Step Simulate Synthetic Data T·ª´ Dataset Moodle

D·ª±a tr√™n chi·∫øn l∆∞·ª£c hybrid t√¥i ph√¢n t√≠ch tr∆∞·ªõc (GMM cho impose 3 c·ª•m tr√™n features [actions/user, finalgrade] ƒë·ªÉ gi·ªØ corr 0.75, rule-based bias theo c·ª•m cho logs, NB cho over-dispersion actions, Beta cho skewed grades), d∆∞·ªõi ƒë√¢y l√† h∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ simulate. Quy tr√¨nh d√πng **Python (scikit-learn, scipy, pandas)**, d·ªÖ ch·∫°y trong Jupyter Notebook. Gi·∫£ s·ª≠ b·∫°n c√≥ hai file CSV (logs v√† grades) ·ªü th∆∞ m·ª•c hi·ªán t·∫°i.

**M·ª•c ti√™u**: T·∫°o synthetic dataset x10 l·ªõn (140k logs, 2.3k grades), gi·ªØ ph√¢n ph·ªëi g·ªëc (probs events ~80% viewed, hourly peak 11-13h, var actions cao, mean grade 7.64), nh∆∞ng v·ªõi 3 c·ª•m r√µ (gi·ªèi: high actions/grade; kh√°: medium; y·∫øu: low) ‚Äì silhouette >0.4.

**Y√™u c·∫ßu m√¥i tr∆∞·ªùng**: `pip install scikit-learn scipy pandas numpy matplotlib seaborn` (n·∫øu ch∆∞a c√≥). Ch·∫°y t·ª´ng step trong notebook ƒë·ªÉ debug.

#### Step 1: Chu·∫©n B·ªã Data V√† T√≠nh Features (Load & Merge)
- **M√¥ t·∫£**: Load hai file, t√≠nh actions/user t·ª´ logs, merge v·ªõi finalgrade theo userid. Scale features cho GMM (v√¨ var actions >> grade). ƒê√¢y l√† base ƒë·ªÉ fit GMM.
- **L√Ω do**: Merge capture corr (0.75), scale tr√°nh bias over-dispersion.
- **Code** (Ch·∫°y cell n√†y ƒë·∫ßu ti√™n):
  ```python
  import pandas as pd
  import numpy as np
  from sklearn.preprocessing import StandardScaler
  from sklearn.mixture import GaussianMixture
  from sklearn.metrics import silhouette_score
  from scipy import stats
  import matplotlib.pyplot as plt
  import seaborn as sns

  # Load files
  log_df = pd.read_csv('udk_moodle_log_course_670.csv', parse_dates=['timecreated'])
  grades_df = pd.read_csv('udk_moodle_grades_course_670.csv')

  # T√≠nh features t·ª´ logs
  user_actions = log_df.groupby('userid').size().to_frame('actions')

  # Merge v·ªõi grades (avg finalgrade n·∫øu multi per user)
  avg_grades = grades_df.groupby('userid')['finalgrade'].mean().to_frame('finalgrade')
  features_df = pd.merge(user_actions, avg_grades, left_index=True, right_index=True, how='inner')
  features_df = features_df.fillna({'finalgrade': features_df['finalgrade'].mean()})  # Fill n·∫øu null

  # Scale features
  scaler = StandardScaler()
  X_scaled = scaler.fit_transform(features_df[['actions', 'finalgrade']])

  print("Features shape:", features_df.shape)
  print(features_df.head())
  print("Corr check:", features_df['actions'].corr(features_df['finalgrade']))
  ```
- **Output mong ƒë·ª£i**: DataFrame ~233 rows (users overlap), corr ~0.75.
- **Ki·ªÉm tra**: N·∫øu merge <100 users, adjust how='outer' v√† fill 0.

#### Step 2: Fit GMM ƒê·ªÉ Impose 3 C·ª•m (Joint Distribution)
- **M√¥ t·∫£**: Fit GMM tr√™n X_scaled ƒë·ªÉ h·ªçc joint dist (capture corr), v·ªõi 3 components (impose gi·ªèi/kh√°/y·∫øu). EM s·∫Ω t·ª± assign means (e.g., gi·ªèi: actions cao, grade ~9).
- **L√Ω do**: Corr m·∫°nh ‚Üí GMM gi·ªØ relation; n_components=3 l√†m r√µ c·ª•m d√π g·ªëc over-dispersed.
- **Code** (Ti·∫øp theo Step 1):
  ```python
  # Fit GMM
  gmm = GaussianMixture(n_components=3, random_state=42)
  gmm.fit(X_scaled)

  # Predict labels g·ªëc (soft/hard assignment)
  labels_orig = gmm.predict(X_scaled)
  sil_orig = silhouette_score(X_scaled, labels_orig)
  print(f"Silhouette g·ªëc: {sil_orig:.3f} (th·∫•p n·∫øu l·ªôn x·ªôn)")

  # Plot clusters g·ªëc
  features_df['cluster_orig'] = labels_orig
  sns.scatterplot(data=features_df, x='actions', y='finalgrade', hue='cluster_orig')
  plt.title('G·ªëc Clusters T·ª´ GMM')
  plt.show()
  ```
- **Output mong ƒë·ª£i**: Silhouette ~0.2-0.4 (g·ªëc l·ªôn x·ªôn), plot scatter v·ªõi 3 m√†u ph√¢n t√°ch nh·∫π.
- **Ki·ªÉm tra**: N·∫øu sil <0, th·ª≠ n_components=2 ho·∫∑c features th√™m (e.g., hour mean).

#### Step 3: Generate Synthetic Features/Users V·ªõi 3 C·ª•m (Sample T·ª´ GMM)
- **M√¥ t·∫£**: Sample x10 users t·ª´ GMM (2.3k), unscale ƒë·ªÉ gi·ªØ range g·ªëc. Assign labels d·ª±a mean comp (gi·ªèi: mean cao nh·∫•t).
- **L√Ω do**: Sample gi·ªØ mean/var/corr; x10 scale volume.
- **Code** (Ti·∫øp theo Step 2):
  ```python
  # Generate synthetic
  n_synth = len(features_df) * 10
  X_synth_scaled, probs = gmm.sample(n_synth)  # probs: soft assignment
  X_synth = scaler.inverse_transform(X_synth_scaled)

  # DataFrame synthetic
  synth_df = pd.DataFrame(X_synth, columns=['actions', 'finalgrade'])
  synth_df['userid'] = range(10000, 10000 + n_synth)  # Synthetic IDs

  # Assign hard labels
  synth_df['cluster'] = np.argmax(probs, axis=1)
  cluster_means = synth_df.groupby('cluster')[['actions', 'finalgrade']].mean()
  sorted_clusters = cluster_means.mean(axis=1).sort_values(ascending=False).index
  cluster_names = {sorted_clusters[0]: 'gi·ªèi', sorted_clusters[1]: 'kh√°', sorted_clusters[2]: 'y·∫øu'}
  synth_df['group'] = synth_df['cluster'].map(cluster_names)

  # Validate clusters
  labels_synth = synth_df['cluster'].values
  sil_synth = silhouette_score(X_synth_scaled, labels_synth)
  print(f"Silhouette synthetic: {sil_synth:.3f} (>0.4: r√µ r√†ng)")
  print("Ph√¢n b·ªë nh√≥m:", synth_df['group'].value_counts())
  print("Corr synthetic:", synth_df['actions'].corr(synth_df['finalgrade']))

  # Plot
  sns.scatterplot(data=synth_df, x='actions', y='finalgrade', hue='group')
  plt.title('Synthetic Clusters (3 Nh√≥m)')
  plt.show()
  ```
- **Output mong ƒë·ª£i**: Sil ~0.5+ (r√µ h∆°n g·ªëc), corr ~0.75, ~770 users/nh√≥m (balanced weights).
- **Ki·ªÉm tra**: N·∫øu corr l·ªách >0.1, re-fit GMM v·ªõi covariance_type='full'.

#### Step 4: Generate Synthetic Logs T·ª´ Features (Rule-Based Bias Theo C·ª•m)
- **M√¥ t·∫£**: D√πng probs g·ªëc (80% viewed) cho events/actions, nh∆∞ng bias theo group (gi·ªèi: tƒÉng updated 20%). S·ªë actions/user ~ NB (fit t·ª´ mean/var g·ªëc). Time theo hourly probs.
- **L√Ω do**: Logs sequences; NB capture over-dispersion (var>>mean).
- **Code** (Ti·∫øp theo Step 3; fit NB tr∆∞·ªõc):
  ```python
  from scipy.stats import nbinom  # Negative Binomial

  # Fit NB cho actions (r, p t·ª´ mean/var g·ªëc)
  mean_orig = user_actions.mean()  # 636.14
  var_orig = user_actions.var()    # 293456
  r_nb = mean_orig**2 / (var_orig - mean_orig)  # ~0.14 (low r = over-disp)
  p_nb = r_nb / (r_nb + mean_orig)
  print(f"NB params: r={r_nb:.2f}, p={p_nb:.4f}")

  # Probs g·ªëc t·ª´ ph√¢n t√≠ch tr∆∞·ªõc (hardcode t·ª´ output)
  probs_action = {'viewed': 0.801, 'updated': 0.068, 'graded': 0.038, 'uploaded': 0.028, 'created': 0.028, 'submitted': 0.018}  # Normalize n·∫øu c·∫ßn
  probs_hour = {11: 0.096, 12: 0.093, 13: 0.083, 16: 0.069, 15: 0.068}  # Top, fill rest uniform
  probs_hour = {h: probs_hour.get(h, 1/len(range(24))) for h in range(24)}  # Normalize

  # Generate logs
  synthetic_logs = []
  start_date = pd.Timestamp('2022-09-01')
  for _, row in synth_df.iterrows():
      num_actions = max(1, int(nbinom.rvs(r_nb, p_nb)))  # NB sample, min 1
      group = row['group']
      
      # Bias probs theo group
      if group == 'gi·ªèi':
          probs_action_bias = probs_action.copy()
          probs_action_bias['updated'] += 0.20  # TƒÉng active
          probs_action_bias['viewed'] -= 0.10
      elif group == 'y·∫øu':
          probs_action_bias = probs_action.copy()
          probs_action_bias['viewed'] += 0.10
          probs_action_bias['updated'] -= 0.05
      else:  # kh√°
          probs_action_bias = probs_action.copy()
      
      # Normalize bias probs
      total_bias = sum(probs_action_bias.values())
      probs_action_bias = {k: v/total_bias for k, v in probs_action_bias.items()}
      
      for _ in range(num_actions):
          action = np.random.choice(list(probs_action_bias.keys()), p=list(probs_action_bias.values()))
          eventname = '\\mod_assign\\event\\course_module_viewed' if action == 'viewed' else '\\mod_quiz\\event\\attempt_started'  # Map ƒë∆°n gi·∫£n
          hour = np.random.choice(list(probs_hour.keys()), p=list(probs_hour.values()))
          timecreated = start_date + pd.Timedelta(days=np.random.randint(365), hours=hour, minutes=np.random.randint(60))
          userid = row['userid']
          courseid = 670
          other = "{'assignid': '****'}" if np.random.rand() > 0.5 else np.nan
          
          synthetic_logs.append({
              'id': np.random.randint(9000000, 10000000),  # Fake ID
              'timecreated': timecreated,
              'eventname': eventname,
              'action': action,
              'target': 'course_module',
              'userid': userid,
              'courseid': courseid,
              'other': other
          })

  synth_logs_df = pd.DataFrame(synthetic_logs)
  print("Synthetic logs shape:", synth_logs_df.shape)
  synth_logs_df.to_csv('synthetic_logs.csv', index=False)
  ```
- **Output mong ƒë·ª£i**: ~140k rows, probs action bias (gi·ªèi: updated ~27%).
- **Ki·ªÉm tra**: Counter(synth_logs_df['action']). T·ª∑ l·ªá ~80% viewed t·ªïng.

#### Step 5: Generate Synthetic Grades T·ª´ Features (Beta Cho Marginal)
- **M√¥ t·∫£**: Assign finalgrade t·ª´ GMM, timemodified = timecreated cu·ªëi + random delay. Itemtype='course' fixed.
- **L√Ω do**: Gi·ªØ skewed (Beta fit mean=7.64, scale 0-10; -1 map to 0).
- **Code** (Ti·∫øp theo Step 4):
  ```python
  # Fit Beta cho grades (scale 0-10, mean=7.64, var=2.95^2~8.7)
  # Method of moments: Œ± = mean^2 * (scale-mean) / var * scale, Œ≤ = Œ± * (scale - mean) / mean
  scale = 10
  mean_g = 7.64  # Clip -1 to 0 tr∆∞·ªõc fit
  var_g = (2.95)**2
  alpha = (mean_g**2 * (scale - mean_g)) / var_g
  beta = alpha * (scale - mean_g) / mean_g
  print(f"Beta params: Œ±={alpha:.2f}, Œ≤={beta:.2f}")

  # Generate grades
  synth_grades = []
  for _, row in synth_df.iterrows():
      finalgrade = np.clip(stats.beta.rvs(alpha, beta, size=1)[0] * scale, 0, 10)  # Clip -1 equiv to 0
      timemodified = pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(1, 30))  # Random update
      synth_grades.append({
          'id': np.random.randint(300000, 400000),
          'timemodified': timemodified,
          'userid': row['userid'],
          'courseid': 670,
          'finalgrade': finalgrade[0],
          'itemtype': 'course'
      })

  synth_grades_df = pd.DataFrame(synth_grades)
  print("Synthetic grades shape:", synth_grades_df.shape)
  synth_grades_df.to_csv('synthetic_grades.csv', index=False)

  # Quick check
  print(f"Synthetic grades mean: {synth_grades_df['finalgrade'].mean():.2f}")
  ```
- **Output mong ƒë·ª£i**: ~2.3k rows, mean ~7.64.
- **Ki·ªÉm tra**: Hist so v·ªõi g·ªëc.

#### Step 6: Validation T·ªïng Th·ªÉ (Fidelity & Clusters)
- **M√¥ t·∫£**: Ch·∫°y tests so s√°nh g·ªëc/synthetic (KS cho probs, Pearson corr, silhouette).
- **L√Ω do**: ƒê·∫£m b·∫£o <10% error (nh∆∞ Moreno paper).
- **Code** (Cu·ªëi):
  ```python
  # KS for action probs
  from scipy.stats import ks_2samp
  orig_actions = log_df['action'].value_counts(normalize=True)
  synth_actions = synth_logs_df['action'].value_counts(normalize=True)
  ks_stat, ks_p = ks_2samp(orig_actions.values, synth_actions.reindex(orig_actions.index, fill_value=0).values)
  print(f"Action KS p: {ks_p:.4f} (>0.05: gi·ªëng)")

  # Corr synthetic
  synth_corr = synth_df['actions'].corr(synth_df['finalgrade'])
  print(f"Synth corr: {synth_corr:.3f} (g·∫ßn 0.75)")

  # Silhouette
  print(f"Synth sil: {sil_synth:.3f}")

  # Plot so s√°nh grades
  fig, ax = plt.subplots(1, 2, figsize=(10,4))
  grades_df['finalgrade'].hist(ax=ax[0], alpha=0.7, label='Orig')
  synth_grades_df['finalgrade'].hist(ax=ax[1], alpha=0.7, label='Synth')
  ax[0].set_title('Orig Grades')
  ax[1].set_title('Synth Grades')
  plt.show()
  ```
- **Output mong ƒë·ª£i**: KS p>0.05, corr~0.75, sil>0.4.
- **Ki·ªÉm tra**: N·∫øu l·ªách, tune bias (e.g., +5% updated cho gi·ªèi).

Ho√†n t·∫•t! Ch·∫°y full notebook, b·∫°n c√≥ synthetic CSV s·∫µn d√πng (e.g., cho JMeter replay). N·∫øu error, debug Step 1. Mu·ªën JMeter integrate (script JMX), h·ªèi th√™m nh√©! üòä