# Q-Table Update - Quick Reference

## âœ… ÄÃ£ HoÃ n ThÃ nh

### 1. **QTableUpdateService** - Core Update Logic
```python
from services.qtable_update_service import QTableUpdateService

updater = QTableUpdateService(
    agent=qlearning_agent,
    reward_calculator=reward_calc,
    action_space=action_space,
    log_to_state_builder=builder
)

stats = updater.update_from_logs(logs)
# â†’ {users_processed, transitions, q_updates, avg_reward, action_counts}
```

### 2. **LogProcessingPipeline** - Integrated Pipeline
```python
from pipeline.log_processing_pipeline import LogProcessingPipeline

pipeline = LogProcessingPipeline(
    cluster_profiles_path='...',
    course_structure_path='...',
    qtable_updater=updater,
    enable_qtable_updates=True  # â† NEW!
)

summary = pipeline.process_logs_from_dict(raw_logs)
print(f"Q-updates: {summary['qtable_updates']}")
```

---

## ğŸš€ Quick Start

```bash
# Test standalone
python3 services/qtable_update_service.py

# Run complete demo
python3 demo_complete_pipeline.py
```

**Demo Output**:
```
âœ… 19 logs processed
âœ… 3 states built  
âœ… 12 transitions detected
âœ… 12 Q-table updates
âœ… Avg reward: 1.062
âœ… Best action learned: submit_quiz (Q=0.675)
```

---

## ğŸ”„ Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Moodle Logs â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LogToStateBuilderâ”‚ â† Existing
â”‚ â†’ 6D States      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QTableUpdateService    â”‚ â† NEW!
â”‚ - Detect transitions   â”‚
â”‚ - Map actions          â”‚
â”‚ - Calculate rewards    â”‚
â”‚ - Update Q(s,a)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QLearningAgentV2â”‚ â† agent.update()
â”‚ Q-table updated â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Key Features

### 1. State Transition Detection
- âœ… Sort logs by (user, timestamp)
- âœ… Filter by time gap (60s - 3600s)
- âœ… Validate state existence
- âœ… Map actions to indices

### 2. Action Mapping
```python
'view_content'   â†’ view_content (past)     [idx=0]
'attempt_quiz'   â†’ attempt_quiz (past)     [idx=5]
'submit_quiz'    â†’ submit_quiz (current)   [idx=9]
'review_errors'  â†’ review_quiz (past)      [idx=7]
```

### 3. Reward Calculation
```python
action_dict = {
    'type': 'attempt_quiz',
    'difficulty': 'medium',
    'expected_time': 300
}

outcome_dict = {
    'completed': True,
    'score': 0.75,
    'success': True
}

reward = reward_calc.calculate_reward(state, action_dict, outcome_dict)
```

### 4. Q-Learning Update
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]

Cluster-adaptive Î±:
- Weak (C0): 0.15
- Medium (C1-2): 0.10
- Strong (C3-4): 0.08
```

---

## ğŸ“ˆ Usage Scenarios

### Scenario 1: Real-time Updates
```python
@app.route('/webhook/logs', methods=['POST'])
def receive_logs():
    logs = request.json['logs']
    stats = pipeline.process_logs_from_dict(logs)
    return {'q_updates': stats['qtable_updates']}
```

### Scenario 2: Batch Training
```python
# Daily training job
historical_logs = load_logs(yesterday)
stats = updater.update_from_logs(historical_logs)
agent.save_q_table('models/qtable_daily.pkl')
```

### Scenario 3: Per-User Updates
```python
user_logs = fetch_user_logs(user_id=101)
stats = updater.update_from_logs(user_logs, user_id=101)
print(f"User 101: {stats['q_updates']} updates")
```

---

## ğŸ§ª Testing Results

### Demo Statistics
| Metric | Value |
|--------|-------|
| Logs processed | 19 |
| States built | 3 |
| Valid transitions | 12 (63%) |
| Q-updates | 12 |
| Total reward | 12.750 |
| Avg reward | 1.062 |

### Top Q-Values
| State | Action | Q-Value |
|-------|--------|---------|
| C2\|M0\|0.75\|0.75 | submit_quiz | 0.6750 |
| C2\|M0\|0.75\|0.75 | attempt_quiz | 0.3990 |
| C2\|M0\|0.50\|1.00 | attempt_quiz | 0.1548 |

### Action Distribution
- `view_content (past)`: 7 times
- `attempt_quiz (past)`: 4 times  
- `submit_quiz (current)`: 1 time

---

## âš™ï¸ Configuration

```python
updater = QTableUpdateService(
    agent=agent,
    reward_calculator=reward_calc,
    action_space=action_space,
    log_to_state_builder=builder,
    
    # Optional parameters
    lo_mastery_tracker=None,          # LO mastery bonus
    min_transition_gap=60,            # Min 1 minute
    max_transition_gap=3600,          # Max 1 hour
    verbose=False                     # Debug logging
)
```

---

## ğŸ¯ Next Steps

1. **Connect Moodle API**
```python
pipeline = LogProcessingPipeline(
    ...,
    moodle_url='https://moodle.com',
    moodle_token='TOKEN',
    course_id=123
)
```

2. **Enable MongoDB**
```python
pipeline = LogProcessingPipeline(
    ...,
    mongo_uri='mongodb+srv://...'
)
```

3. **Production Deployment**
```python
# API endpoint
@app.route('/api/recommend')
def recommend():
    state = get_current_state(user_id)
    action = agent.select_action(state)
    return {'action': action}
```

---

## ğŸ“ Files Created

```
services/qtable_update_service.py      - Core update logic (520 lines)
demo_complete_pipeline.py              - Full demo (322 lines)
Q_TABLE_UPDATE_GUIDE.md               - Detailed guide
Q_TABLE_UPDATE_README.md              - This file
```

---

## ğŸ“ Key Takeaways

âœ… **Before**: Static pipeline (logs â†’ states)  
âœ… **Now**: Learning pipeline (logs â†’ states â†’ Q-updates)

âœ… **Impact**:
- Online learning from real behavior
- Adaptive recommendations
- Continuous improvement

âœ… **Production Ready**:
- Tested with realistic data
- Configurable parameters
- Error handling
- Statistics tracking

ğŸ‰ **System is now a complete Q-Learning pipeline!**
