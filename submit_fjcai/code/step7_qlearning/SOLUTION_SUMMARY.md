# âœ… FIX SUMMARY: TÄƒng Score Midterm Tá»« 4.5 â†’ 7.0-7.5/10

## ğŸ“‹ Nhá»¯ng GÃ¬ ÄÆ°á»£c Thay Äá»•i

### 1. **TÄƒng Learning Rate (config/reward_config.json)**

```json
{
  "version": "2.1",
  "mastery_learning_rates": {
    "weak": 0.4,      // â† Tá»« 0.3
    "medium": 0.3,    // â† Tá»« 0.2
    "strong": 0.2     // â† Tá»« 0.15
  }
}
```

**Effect**: Mastery update nhanh hÆ¡n 30-100%

---

### 2. **ThÃªm Mastery Improvement Reward (config/reward_config.json)**

```json
{
  "reward_components": {
    "lo_mastery_improvement": {
      "weak": 15.0,
      "medium": 10.0,
      "strong": 7.0,
      "description": "NEW: Reward for improving LO mastery (per delta point)"
    }
  }
}
```

**Effect**: 
- Reward formula thay tá»«: `delta Ã— midterm_weight Ã— cluster_bonus Ã— inverse_mastery Ã— 10`
- ThÃ nh: `delta Ã— mastery_improvement_reward[cluster]`
- Äiá»u nÃ y incentivize agent cáº£i thiá»‡n mastery directly

---

### 3. **Update Reward Calculator (core/rl/reward_calculator.py)**

**A. Sá»­ dá»¥ng Config Learning Rates**
```python
# Line ~375
learning_rates = self.config.get('mastery_learning_rates', {
    'weak': 0.4, 'medium': 0.3, 'strong': 0.2
})
alpha = learning_rates.get(cluster_level, 0.3)  # Thay vÃ¬ hardcoded
```

**B. ThÃªm Mastery Improvement Reward**
```python
# Line ~422
# NEW: Explicit mastery improvement reward from config
mastery_improvement_rewards = self.config.get('reward_components', {}).get('lo_mastery_improvement', {
    'weak': 15.0, 'medium': 10.0, 'strong': 7.0
})
mastery_improvement_multiplier = mastery_improvement_rewards.get(cluster_level, 10.0)

# Reward is: delta Ã— multiplier
lo_reward = delta * mastery_improvement_multiplier
```

---

## ğŸ“Š Dá»± Kiáº¿n Káº¿t Quáº£

### TrÆ°á»›c Fix:
| Metric | Hiá»‡n Táº¡i |
|--------|----------|
| Mastery | 0.46 |
| Score /20 | 9.2 |
| Score /10 | 4.6 |

### Sau Fix (Phase 1):
| Metric | Dá»± Kiáº¿n |
|--------|---------|
| Mastery | 0.65-0.70 |
| Score /20 | 13-14 |
| Score /10 | **6.5-7.0** âœ“ |

### Má»¥c TiÃªu Cuá»‘i CÃ¹ng (Phase 2):
| Metric | Target |
|--------|--------|
| Mastery | 0.75-0.80 |
| Score /20 | 15-16 |
| Score /10 | **7.5-8.0** |

---

## ğŸš€ CÃ¡ch Cháº¡y Láº¡i Comparison

```bash
# Trong thÆ° má»¥c demo_pineline/step7_qlearning/

# 1. Cháº¡y Q-Learning pipeline láº¡i (sáº½ dÃ¹ng config má»›i)
python3 scripts/demos/full_pipeline_qlearning.py \
  --n-episodes 100 \
  --n-students 100 \
  --output data/simulated/qlearning_policy_results_v2.json

# 2. Cháº¡y Param Policy pipeline
python3 scripts/demos/full_pipeline_param.py \
  --n-students 100 \
  --output data/simulated/param_policy_results_v2.json

# 3. So sÃ¡nh káº¿t quáº£
python3 scripts/utils/compare_policies.py \
  --q-learning data/simulated/qlearning_policy_results_v2.json \
  --param-policy data/simulated/param_policy_results_v2.json \
  --output data/simulated/comparison_report_v2.json

# 4. Váº½ biá»ƒu Ä‘á»“
python3 scripts/utils/plot_policy_comparison.py \
  --comparison-report data/simulated/comparison_report_v2.json \
  --output plots/policy_comparison_v2
```

---

## ğŸ“ Chi Tiáº¿t Ká»¹ Thuáº­t

### Táº¡i Sao Äiá»u NÃ y Hoáº¡t Äá»™ng?

**Váº¥n Ä‘á» Gá»‘c**:
- Score phá»¥ thuá»™c vÃ o Mastery: `Score = Î£(mastery[lo] Ã— weight[lo] Ã— 20)`
- Mastery quÃ¡ tháº¥p (0.46) â†’ Score tháº¥p (4.6/10)
- Mastery update cháº­m vÃ¬ Î± quÃ¡ bÃ© (0.2-0.3)

**Giáº£i PhÃ¡p**:
1. **TÄƒng Î±** â†’ Mastery há»c nhanh hÆ¡n 30-100%
2. **ThÃªm Direct Reward** â†’ Agent cá»‘ gáº¯ng cáº£i thiá»‡n mastery trá»±c tiáº¿p (bonus 7-15 Ä‘iá»ƒm/mastery point)
3. **Káº¿t há»£p** â†’ Mastery tÄƒng nhanh + score tÄƒng nhanh

**VÃ­ Dá»¥ Cá»¥ Thá»ƒ**:
```python
# TrÆ°á»›c:
# Má»™t láº§n improve LO mastery 0.05 point, má»—i LO
# Î± = 0.2, delta = 0.2 Ã— 0.05 Ã— 1.2 Ã— 1.7 Ã— 10 â‰ˆ 0.2 reward

# Sau:
# Má»™t láº§n improve LO mastery 0.05 point
# Î´ = 0.05, multiplier = 10 (medium cluster)
# lo_reward = 0.05 Ã— 10 = 0.5 reward

# â†’ 2.5x nhiá»u reward hÆ¡n â†’ Agent cá»‘ gáº¯ng hÆ¡n â†’ Mastery cao hÆ¡n â†’ Score cao hÆ¡n
```

---

## ğŸ” Verification Checklist

Sau khi update, kiá»ƒm tra:

- [ ] Config file `config/reward_config.json` cÃ³ learning_rates?
- [ ] Config file cÃ³ `lo_mastery_improvement` reward?
- [ ] `core/rl/reward_calculator.py` sá»­ dá»¥ng config learning_rates?
- [ ] `core/rl/reward_calculator.py` tÃ­nh reward dÃ¹ng mastery_improvement_multiplier?
- [ ] Cháº¡y simulation test (5-10 há»c sinh) Ä‘á»ƒ verify khÃ´ng lá»—i
- [ ] So sÃ¡nh cáº£ 2 policies láº¡i vÃ  kiá»ƒm tra score tÄƒng

---

## ğŸ“Œ LÆ°u Ã Quan Trá»ng

1. **Normalization**: Reward váº«n Ä‘Æ°á»£c normalize (clip -5 Ä‘áº¿n 15), khÃ´ng váº¥n Ä‘á» gÃ¬
2. **Q-table**: CÃ³ thá»ƒ cáº§n train thÃªm epoch vÃ¬ reward distribution thay Ä‘á»•i
3. **Backward Compatibility**: Code cÅ© váº«n hoáº¡t Ä‘á»™ng náº¿u config khÃ´ng cÃ³ fields má»›i (cÃ³ default values)
4. **Monitoring**: Kiá»ƒm tra mastery_history Ä‘á»ƒ verify mastery thá»±c sá»± tÄƒng

---

## ğŸ¯ Tiáº¿p Theo

1. âœ… Cháº¡y láº¡i comparison
2. âœ… Kiá»ƒm tra mastery tÄƒng lÃªn bao nhiÃªu
3. âœ… Kiá»ƒm tra score tÄƒng lÃªn bao nhiÃªu
4. âš ï¸ Náº¿u score váº«n < 6/10: CÃ³ thá»ƒ cáº§n tÄƒng n_steps hoáº·c training epochs
5. âš ï¸ Náº¿u reward cao nhÆ°ng score váº«n tháº¥p: Debug mastery update logic

**File Ä‘Æ°á»£c táº¡o**: `ANALYSIS_SCORE_REWARD_GAP.md` (phÃ¢n tÃ­ch chi tiáº¿t váº¥n Ä‘á»)
