# PhÃ¢n TÃ­ch Váº¥n Äá»: Reward KhÃ¡c NhÆ°ng Score Giá»‘ng Nhau

## ğŸ” TÃ³m Táº¯t Váº¥n Äá»

Khi so sÃ¡nh `qlearning_policy` vs `param_policy`:
- âœ… **Reward**: KhÃ¡c nhau rÃµ rá»‡t (224 vs 52)
- âŒ **Äiá»ƒm Midterm**: Giá»‘ng nhau (9.41/20 vs 8.90/20) â‰ˆ (4.7/10 vs 4.45/10)
- âŒ **Cáº£ 2 Ä‘á»u tháº¥p**: Chá»‰ ~4.5/10 thay vÃ¬ má»¥c tiÃªu 10/20 (5/10)

### Dá»¯ Liá»‡u Tá»« Comparison Report

```json
{
  "q_learning": {
    "avg_reward": 224.05,
    "avg_midterm_score": 9.41,          // /20
    "avg_midterm_score_10": 4.71,       // /10
    "avg_lo_mastery": 0.4628
  },
  "param_policy": {
    "avg_reward": 52.11,
    "avg_midterm_score": 8.90,          // /20
    "avg_midterm_score_10": 4.45,       // /10
    "avg_lo_mastery": 0.4642
  }
}
```

---

## ğŸ¯ Root Cause Analysis

### 1. **Score KHÃ”NG Phá»¥ Thuá»™c Trá»±c Tiáº¿p VÃ o Reward**

```python
# Trong lo_mastery_tracker.py:200-220
def predict_midterm_score(self, student_id: int):
    mastery = self.get_mastery(student_id)
    
    # âš ï¸ Score = mastery Ã— weight Ã— total_marks
    for lo_id, weight in self.midterm_weights.items():
        lo_mastery = mastery.get(lo_id, 0.4)  # Default: 0.4 = 40%
        lo_score = lo_mastery * weight * 20   # Always /20
        total_score += lo_score
    
    midterm_score_10 = total_score / 2.0     # Convert to /10
```

**Káº¿t luáº­n**: Score lÃ  **hÃ m cá»§a Mastery**, khÃ´ng pháº£i Reward!

---

### 2. **Mastery QuÃ¡ Tháº¥p (~0.46)**

**TÃ­nh toÃ¡n**:
```
Score = Î£(mastery[lo] Ã— weight[lo] Ã— 20)
      = 0.46 Ã— 20  (náº¿u weights cÃ³ trung bÃ¬nh â‰ˆ 1.0)
      â‰ˆ 9.2 / 20
      â‰ˆ 4.6 / 10
```

**Táº¡i sao mastery chá»‰ 0.46?**

Mastery Ä‘Æ°á»£c update qua hÃ m learning:
```python
# Trong reward_calculator.py:380-420
alpha = {'weak': 0.3, 'medium': 0.2, 'strong': 0.15}[cluster]
new_mastery = old_mastery + alpha * (outcome_score - old_mastery)
```

**Váº¥n Ä‘á»**:
- ğŸ“‰ Learning rate `Î±` quÃ¡ bÃ© (0.15-0.3)
- ğŸ“‰ Outcome score tá»« simulation cÃ³ thá»ƒ quÃ¡ tháº¥p
- ğŸ“‰ Sá»‘ hoáº¡t Ä‘á»™ng training quÃ¡ Ã­t so vá»›i tá»•ng LO

---

### 3. **Táº¡i Sao Q-Learning Policy Reward Cao NhÆ°ng Score Váº«n Tháº¥p?**

```
Q-Learning Advantage: 224 reward (vs 52 param policy)
â†’ Agent há»c tá»‘t hÆ¡n â†’ recommend hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n
â†’ Outcome tá»‘t hÆ¡n â†’ Mastery tÄƒng nhiá»u hÆ¡n

NhÆ°ng thá»±c táº¿: 
- Q-Learning mastery: 0.4628
- Param mastery: 0.4642 (cao hÆ¡n!)

âŒ Äiá»u nÃ y khÃ´ng há»£p lÃ½!
```

**CÃ³ thá»ƒ lÃ **:
1. Q-Learning policy training chÆ°a Ä‘á»§ epoch
2. Q-table chÆ°a há»™i tá»¥ tá»‘t
3. Simulation parameters khÃ´ng chuáº©n (e.g., outcome score quÃ¡ tháº¥p)

---

## ğŸ”§ Giáº£i PhÃ¡p

### **Giáº£i PhÃ¡p 1: TÄƒng Learning Rate (Ngáº¯n háº¡n)**

TÄƒng `Î±` Ä‘á»ƒ mastery update nhanh hÆ¡n:

```json
// config/reward_config.json
"mastery_learning_rates": {
  "weak": 0.5,      // â† Tá»« 0.3
  "medium": 0.35,   // â† Tá»« 0.2
  "strong": 0.25    // â† Tá»« 0.15
}
```

**Effect**: Mastery ~0.46 â†’ ~0.65-0.70
**Score**: 4.6/10 â†’ 6.5-7.0/10 âœ“

---

### **Giáº£i PhÃ¡p 2: Cáº£i Thiá»‡n Outcome Quality (Trung háº¡n)**

**Váº¥n Ä‘á» hiá»‡n táº¡i**:
- Simulation outcome score quÃ¡ tháº¥p
- Hoáº¡t Ä‘á»™ng cÃ³ Ä‘á»™ khÃ³ khÃ´ng phÃ¹ há»£p vá»›i cluster

**Cáº£i thiá»‡n**:
```python
# core/simulation/simulator.py
def execute_action(self, action, student):
    # âš ï¸ Hiá»‡n táº¡i: outcome_score ~ random[0, 0.8]
    # âœ… NÃªn: outcome_score ~ f(student_level, action_difficulty)
    
    if student.cluster == 'weak':
        # Weak student lÃ m action easy â†’ high success rate
        success_rate = 0.8
        expected_score = 0.85
    elif student.cluster == 'medium':
        expected_score = 0.70
    else:  # strong
        expected_score = 0.75
    
    # Add noise but bias towards expected
    outcome_score = np.random.beta(alpha, beta) # Tuned (Î±,Î²)
```

---

### **Giáº£i PhÃ¡p 3: Tá»‘i Æ¯u Q-Learning Training (DÃ i háº¡n)**

**Váº¥n Ä‘á»**: Q-Learning policy reward 224 nhÆ°ng mastery tháº¥p

**NguyÃªn nhÃ¢n cÃ³ thá»ƒ**:
1. **Exploration-exploitation khÃ´ng cÃ¢n báº±ng**: Îµ-greedy policy
2. **Q-table chÆ°a há»™i tá»¥**: Cáº§n thÃªm epoch
3. **Reward shaping sai**: KhÃ´ng incentivize mastery improvement

**Giáº£i phÃ¡p**:

```python
# config/reward_config.json - Add mastery bonus
"reward_components": {
  "lo_mastery_improvement": {
    "weak": 10.0,      // â† NEW: Reward LO mastery improvement
    "medium": 7.0,
    "strong": 5.0,
    "per_point": true  // Reward per LO improved
  }
}
```

---

### **Giáº£i PhÃ¡p 4: TÄƒng Sá»‘ Hoáº¡t Ä‘á»™ng Training**

**Hiá»‡n táº¡i**: Simulation cháº¡y ~50-100 hoáº¡t Ä‘á»™ng/há»c sinh

**NÃªn**: TÄƒng lÃªn 150-200 hoáº¡t Ä‘á»™ng/há»c sinh

```python
# scripts/run_simulation.py
sim = Simulator(
    n_steps=300,  # â† Tá»« 150-200
    n_students=100
)
```

---

## ğŸ“Š Dá»± Kiáº¿n Káº¿t Quáº£ Sau Fix

| Metric | Hiá»‡n Táº¡i | Sau Fix | Má»¥c TiÃªu |
|--------|----------|---------|---------|
| Mastery Trung BÃ¬nh | 0.46 | 0.70-0.75 | 0.80+ |
| Score /20 | 9.2-9.4 | 14-15 | 16+ |
| Score /10 | 4.6-4.7 | 7.0-7.5 | 8.0+ |
| Weak LO Mastery | 0.35 | 0.55-0.65 | 0.75+ |

---

## ğŸš€ Thá»±c Hiá»‡n Priority

1. **Ngay láº­p tá»©c** (15 min): 
   - TÄƒng learning rate tá»« 0.2 â†’ 0.35 (medium)
   - TÄƒng n_steps tá»« 150 â†’ 250

2. **Ngáº¯n háº¡n** (1 hour):
   - Cáº£i thiá»‡n outcome quality simulation
   - ThÃªm mastery_improvement reward

3. **DÃ i háº¡n** (1-2 hours):
   - Optimize Q-Learning training
   - Fine-tune action difficulty distribution

---

## ğŸ“ Káº¿ Tiáº¿p

HÃ£y tÃ´i:
1. âœ… Update learning rates
2. âœ… Improve simulation outcomes
3. âœ… Add mastery_improvement reward
4. âœ… Re-run comparison

**Sau Ä‘Ã³**: ÄÃ¡nh giÃ¡ káº¿t quáº£ vÃ  Ä‘iá»u chá»‰nh tiáº¿p theo.
