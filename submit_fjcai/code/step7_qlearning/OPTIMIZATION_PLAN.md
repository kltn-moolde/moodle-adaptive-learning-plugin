# ğŸ“ˆ OPTIMIZATION PLAN - Phase 2

## ğŸ¯ Objective
TÄƒng score tá»« 4.76/10 â†’ 7.0+/10 (má»¥c tiÃªu: 5/10 â†’ 8/10)

## âš ï¸ Problem Analysis

Káº¿t quáº£ hiá»‡n táº¡i:
- Score: 4.76/10 (tÄƒng tá»« 4.71 nhÆ°ng chá»‰ +1%)
- Weak LOs: 93-100% há»c sinh váº«n weak
- Reward tÄƒng nháº¹ (224 â†’ 226)

**Root cause**: 
1. Q-table trained vá»›i config cÅ© (500 episodes)
2. Cáº§n retrain vá»›i config má»›i (learning rates + mastery reward)
3. Simulation steps quÃ¡ Ã­t (30 steps)

## ğŸ”§ Optimization Strategy

### Strategy 1: Retrain Q-table (HIGH IMPACT)

**Current**: 500 episodes, reward config cÅ©
**Target**: 1000+ episodes, reward config má»›i (v2.1)

```bash
python3 training/train_qlearning.py \
  --episodes 1000 \
  --total-students 100 \
  --cluster-mix 0.2 0.6 0.2 \
  --course-id 5 \
  --steps 50 \
  --output models/qtable_v2_1000.pkl \
  --plot
```

**Expected result**:
- Q-table há»c vá»›i reward strategy má»›i
- Mastery reward 7-15x lá»›n hÆ¡n â†’ agent cá»‘ gáº¯ng cáº£i thiá»‡n mastery
- Score dá»± kiáº¿n: 5.5-6.5/10

---

### Strategy 2: Increase Simulation Steps (MEDIUM IMPACT)

**Current**: 30 steps/há»c sinh
**Target**: 50-100 steps/há»c sinh

```bash
python3 scripts/utils/simulate_learning_path.py \
  --qtable models/qtable_v2_1000.pkl \
  --output data/simulated/qlearning_policy_v2.json \
  --num_students 100 \
  --cluster-mix 0.2 0.6 0.2 \
  --steps 100 \
  --plot
```

**Expected result**:
- Má»—i há»c sinh cÃ³ 100 bÆ°á»›c Ä‘á»ƒ improve LO
- Score dá»± kiáº¿n: 6.0-7.0/10

---

### Strategy 3: Aggressive Learning Rates (EXPERIMENTAL)

**Current**: weak=0.4, medium=0.3, strong=0.2
**Target**: weak=0.5, medium=0.4, strong=0.3

```json
{
  "mastery_learning_rates": {
    "weak": 0.5,
    "medium": 0.4,
    "strong": 0.3
  }
}
```

**Effect**: Mastery update 25% nhanh hÆ¡n
**Risk**: CÃ³ thá»ƒ overshoot mastery (> 1.0)

---

## ğŸ“Š Testing Plan

| Phase | Action | Time | Expected Score |
|-------|--------|------|-----------------|
| 0 (Current) | None | - | 4.76/10 |
| 1 | Retrain Q (1000 ep) | 30 min | 5.5-6.5/10 |
| 2 | +100 steps | 10 min | 6.0-7.0/10 |
| 3 | +Aggressive rates | 40 min | 7.0-7.5/10 |
| Final | All combined | 90 min | 7.5+/10 |

---

## ğŸš€ Recommended Next Steps

### Immediate (10 min):
1. Retrain Q-table vá»›i 1000 episodes
2. Cháº¡y simulation vá»›i steps=100
3. Check score

### If score < 6/10:
1. Thá»­ aggressive learning rates
2. Coi xÃ©t other reward components

### If score >= 6/10:
1. Optimize fine-tuning
2. Analysis mastery distribution
3. Compare policies (Q-learning vs Param)

---

## ğŸ” Monitoring Metrics

Track cÃ¡c metrics nÃ y:

1. **Q-table convergence**: avg Q-value change per episode
2. **Reward distribution**: mean, std, min, max
3. **Mastery improvement**: Î”mastery per student
4. **Score distribution**: per cluster, per LO
5. **Weak LO rate**: % há»c sinh weak trÃªn má»—i LO

---

## ğŸ“ Notes

- Reward config v2.1 cÃ³ lo_mastery_improvement (7-15)
- Learning rates tÄƒng 30-50%
- Cáº§n retrain Q-table Ä‘á»ƒ cÃ¡ch máº¡ng táº­n dá»¥ng
- 1000 episodes + 100 steps dá»± kiáº¿n Ä‘áº¡t 7.0+/10

---

**Created**: 2025-12-07
**Status**: Ready to Execute
