# ğŸ“ Adaptive STEM Learning Pathway Optimization

<div align="center">

**Personalized STEM Learning via Reinforcement Learning**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Moodle](https://img.shields.io/badge/Moodle-LTI%201.3-orange.svg)](https://docs.moodle.org/dev/LTI)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)

**English** | [Tiáº¿ng Viá»‡t](README_VN.md)

</div>

---

## ğŸ‘¥ Project Information

**Authors:** Nguyen Huu Loc, Van Tuan Kiet

**Supervisor:** Dr. Do Nhu Tai

**Institution:** Faculty of Information Technology - Saigon University

---

## ğŸ“ Abstract

In the context of Education 4.0, traditional Learning Management Systems (LMS) typically apply a uniform learning pathway for all learners, leading to ineffective personalization. This project proposes an **adaptive learning framework** based on **Q-learning** algorithm, integrated into the **Moodle** platform via **LTI 1.3** standard.

The learning process is modeled as a **Markov Decision Process (MDP)**, combined with **K-means** behavioral clustering to construct a multi-dimensional learner state space. Experimental results from 500 simulation episodes demonstrate that the system improves average scores by **22.5%** and reduces weak skills by up to **51.0%**.

**Keywords:** `Reinforcement Learning` â€¢ `Q-learning` â€¢ `Personalized Learning` â€¢ `STEM Education` â€¢ `Moodle LMS` â€¢ `Adaptive Learning`

## ğŸ“Œ Table of Contents

- [ğŸ‘¥ Project Information](#-project-information)
- [ğŸ“ Abstract](#-abstract)
- [ğŸ” Introduction](#-introduction)
- [ğŸ›  Proposed Method](#-proposed-method)
- [ğŸ“Š Experimental Results](#-experimental-results)
- [ğŸ—ï¸ System Architecture](#ï¸-system-architecture)
- [ğŸ’» Installation](#-installation)
- [ğŸ“š References](#-references)
- [ğŸ“„ License](#-license)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“ Contact](#-contact)

---

## ğŸ” Introduction

STEM education faces significant challenges due to **substantial differences** in students' abilities, foundational knowledge, and learning pace. Learning Management Systems (LMS) like Moodle typically function only as content repositories and grade trackers, **lacking behavioral analysis capabilities** and timely pedagogical intervention.

This project proposes an **adaptive learning framework** based on **Reinforcement Learning (Q-learning)** - enabling an AI Agent to autonomously explore and optimize teaching strategies through trial-and-error mechanisms, continuously adapting based on learner feedback.

---

## ğŸ›  Proposed Method

The system models the learning process as a **Markov Decision Process (MDP)** with three components: multi-dimensional state space (6 features), action space (15 pedagogical actions), and multi-objective reward function.

![Proposed Method Overview](image.png)

### ğŸ“ˆ Detailed Methodology

![Detailed Methodology](image-1.png)

### ğŸ”¬ Key Technical Components

#### 1ï¸âƒ£ State Space (S)

**6-dimensional** learner state representation:

| Dimension | Description | Values |
|-----------|-------------|--------|
| **Cluster** | Behavioral cluster (K-means) | 0-4 |
| **Module** | Current learning module | 1-N |
| **Progress** | Completion progress | 0.0-1.0 |
| **Score Level** | Performance level | 0-4 |
| **Phase** | Learning phase (Quiz/Forum/Assignment) | 0-2 |
| **Engagement** | Interaction level | 0-4 |

#### 2ï¸âƒ£ Action Space (A)

**15 pedagogical actions** organized by temporal axis:

- **Past (Remedial):** Review weak Learning Outcomes (LO)
- **Present (Standard):** Follow standard learning pathway
- **Future (Advanced):** Preview advanced content

#### 3ï¸âƒ£ Reward Function (R)

$$R_{total} = R_{base} + R_{LO} + R_{bonus} - P_{penalty}$$

Where:
- $R_{base}$: Base reward from score performance
- $R_{LO}$: Reward for improving weak skills
- $R_{bonus}$: Bonus for active engagement
- $P_{penalty}$: Penalty for inappropriate actions

### ğŸ“ˆ Q-learning Algorithm Details

The Q-learning algorithm uses **Bellman update rule** with epsilon-greedy strategy for exploration-exploitation balance:

$$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$$

Where: $\alpha$ = learning rate (0.1), $\gamma$ = discount factor (0.95)

### ğŸ” Explainable AI (XAI) - SHAP Framework

To interpret Agent decisions, the system integrates **SHAP (SHapley Additive exPlanations)** - measuring each state feature's contribution to action selection:

$$\phi_i(s) = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!}[f(S \cup \{i\}) - f(S)]$$

This helps educators understand **why** the system recommends specific actions for each student.

---

## ğŸ“Š Experimental Results

### âš™ï¸ Experimental Setup

- **Scale:** 500 episodes Ã— 100 virtual students = **50,000 interaction trajectories**
- **Dataset:** Moodle Log & Grades - Course ID 670 (public dataset)
- **Baseline:** Param Policy (historical behavior simulation)
- **Learner modeling:** 70% Linear learners, 20% Video-first, 10% Practice-first

### ğŸ“ˆ Q-table Training Process

![Q-learning Agent Training Process](qtable_growth_states.png)

*Figure: Q-learning convergence over 500 episodes*

### ğŸ“Š Performance Comparison

| Metric | Param Policy (Baseline) | Q-learning (Ours) | Improvement |
|--------|------------------------|-------------------|-------------|
| **Average Score** (scale 0-10) | 5.82 Â± 0.48 | 7.14 Â± 0.82 | â¬†ï¸ **+22.5%** |
| **Weak Skills Count** | 3.02 | 1.48 | â¬‡ï¸ **-51.0%** |
| **Average Reward** | 59.95 Â± 12.38 | 264.26 Â± 27.33 | â¬†ï¸ **+340.8%** |

> ğŸ’¡ **Conclusion:** Q-learning significantly outperforms Param Policy across all metrics, demonstrating its capability to optimize personalized learning pathways.

### ğŸ” Explainability Analysis

![SHAP Analysis - Feature Importance](shap_summary_bar.png)

*Figure: SHAP values reveal that **Cluster** and **Score Level** are the two most important features in the Agent's decision-making process.*

---

## ğŸ—ï¸ System Architecture

### ğŸ“¦ Microservices Overview

![System Architecture](system_design.png)

```
moodle-adaptive-learning-plugin/
â”œâ”€â”€ user-segmentation-service/   # Student behavioral clustering (K-means)
â”œâ”€â”€ course-service/              # Course and content management
â”œâ”€â”€ user-service/                # User information management
â”œâ”€â”€ question-service/            # Question bank management
â”œâ”€â”€ recommend-service/           # Learning recommendation (Q-learning Agent)
â”œâ”€â”€ lti-service-python/          # LTI 1.3 Authentication & Integration
â”œâ”€â”€ FE-service-v3/               # Frontend React + TypeScript
â””â”€â”€ kong-gateway/                # API Gateway & Load Balancer
```

---

## ğŸ’» Installation

### ğŸ“‹ System Requirements

- **Docker & Docker Compose:** 20.10+
- **Moodle:** 4.5+ with LTI 1.3 enabled

### ğŸš€ Docker Deployment

```bash
# Clone repository
git clone https://github.com/kltn-moolde/moodle-adaptive-learning-plugin.git
cd moodle-adaptive-learning-plugin

# Launch entire system
docker compose --env-file .env.production -f docker-compose.prod.yml up -d --pull always --build
```

The system will automatically:
- âœ… Build all microservices
- âœ… Initialize database
- âœ… Configure API Gateway (Kong)
- âœ… Deploy frontend React app

---

## ğŸ“š References

[1] M. T. Chi and R. Wylie, "The ICAP framework: Linking cognitive engagement to active learning outcomes," *Educational Psychologist*, 2014.

[2] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*, MIT Press, 1998.

[3] S. M. Lundberg and S.-I. Lee, "A unified approach to interpreting model predictions," *Advances in Neural Information Processing Systems*, 2017.

[4] IMS Global Learning Consortium, "LTI 1.3 Core Specification," 2019. [Online]. Available: https://www.imsglobal.org/spec/lti/v1p3/

[5] Moodle Documentation, "LTI and Moodle," 2023. [Online]. Available: https://docs.moodle.org/

---

## ğŸ“„ License

This system is released under the **MIT License**.

```
MIT License

Copyright (c) 2026 Nguyen Huu Loc, Van Tuan Kiet

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction...
```

See [LICENSE](LICENSE) for details.

---

## ğŸ¤ Contributing

We welcome contributions to this project!

### ğŸ”§ How to Contribute

1. Fork the repository
2. Create a new branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### ğŸ“ Coding Standards

- Python: Follow PEP 8
- JavaScript/TypeScript: Use ESLint + Prettier
- Commit messages: Conventional Commits format

---

## ğŸ“ Contact

**Research Team:**
- ğŸ“§ Email: [lockbkbang@gmail.com](mailto:lockbkbang@gmail.com)
- ğŸ“± GitHub Issues: [Report bugs](https://github.com/kltn-moolde/moodle-adaptive-learning-plugin/issues)

---

## ğŸ™ Acknowledgments

This project was conducted with support from:
- Faculty of Information Technology - Saigon University
- Dr. Do Nhu Tai (Supervisor)

---

<div align="center">

**â­ If you find this project useful, please give us a star! â­**

Made with â¤ï¸ by Adaptive Learning Team

</div>